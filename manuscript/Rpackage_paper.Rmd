---
title             : "multibridge: An R Package To Evaluate Multinomial Order Constraints" # multibridge: Evaluate Multinomial Order Constraints using the Bayes Factor in R
shorttitle        : "multibridge"

author: 
  - name          : "Alexandra Sarafoglou"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Department of Psychology, PO Box 15906, 1001 NK Amsterdam, The Netherlands"
    email         : "alexandra.sarafoglou@gmail.com"
  - name          : "Julia M. Haaf"
    affiliation   : "1"
  - name          : "Frederik Aust"
    affiliation   : "1"
  - name          : "Eric-Jan Wagenmakers"
    affiliation   : "1"
  - name          : "Maarten Marsman"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "University of Amsterdam"

abstract: |
  The \textbf{multibridge} package efficiently computes Bayes factors for binomial and multinomial models, that feature inequality constraints, equality constraints, free parameters and mixtures between them. By using the bridge sampling algorithm to compute the Bayes factor, \textbf{multibridge} facilitates the evaluation of large models with many constraints and models with small parameter spaces. The package was developed in the R programming language and is freely available from the Comprehensive \texttt{R} Archive Network (CRAN). We illustrate the functions based on two empirical examples.

bibliography      : "../inst/REFERENCES.bib"
appendix          : "Rpackage_appendix.Rmd"

floatsintext      : no
figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
biblio-style      : "apa"
output            : papaja::apa6_pdf
header-includes:
   - \usepackage{bm}
   - \usepackage{amsmath}
   - \usepackage{nicefrac}
   - \usepackage{caption}
   - \usepackage{xcolor}
   - \definecolor{mypink}{RGB}{255, 230, 255}
   - \definecolor{myWheat}{RGB}{245, 222, 179}
   - \definecolor{myGreen}{RGB}{27, 158, 119}
   - \usepackage{todonotes}
   - \newcommand{\Julia}[1]{\todo[inline, color=mypink]{#1}}
   - \newcommand{\Frederik}[1]{\todo[inline, color=myWheat]{#1}}
   - \newcommand{\Alex}[1]{\todo[inline, color=myGreen]{#1}}
---

```{r, echo = FALSE, warning=FALSE}
library(plyr)
library(knitr)
```


# Introduction

We present \textbf{multibridge}, an \texttt{R} package to evaluate informed hypotheses in multinomial models and models featuring independent binomials using Bayesian inference. The package allows users to specify constraints on the underlying category proportions including inequality constraints, equality constraints, free parameters and mixtures between them. The package is available from the Comprehensive \texttt{R} Archive Network (CRAN) at \url{https://CRAN.R-project.org/package=multibridge}. Here we introduce the methodology used to evaluate informed hypotheses on categorical variables and show how to use the implementations provided in \textbf{multibridge} through fully reproducible examples.

The most common way to analyze categorical variables is to test whether the underlying category proportions are exactly equal or whether they are fixed and follow a predicted pattern (what is generally known as either chi-square goodness of fit tests, or binomial or multinomial tests). These null hypotheses are then tested against an encompassing hypothesis which places no constraints on the category proportions. Although commonly used, this analytic strategy has been criticized, since the null hypotheses might reflect an unrealistic expectation about the real world and the encompassing hypothesis is too uninformative [@hoijtink2008bayesian]. In addition, this strategy is often a vague test of the specific predictions that researchers and practitioners are interested in. A simple example for this are theories that predict ordinal relations among the underlying category proportions, such as increasing or decreasing trends. For instance, to check for irregularities in audit data, one could test whether the leading digits in the data are distributed according to an expected Benford distribution or whether they deviate from it, for example, by showing a general decreasing trend. Here, the Benford distribution can be tested with standard methods, however, the general decreasing trend cannot be tested, since we cannot derive fixed underlying proportions for the leading digits. Theories can also generate more complex predictions, including ones that feature combinations of equality and inequality constraints, as well as predictions that let some category proportions free to vary. In the following, we will denote such predictions as informed hypotheses, since they ``add theoretical expectations to the traditional alternative hypothesis, thus making it more informative'' [@hoijtink2008bayesian, p. 2]. Such an informed hypothesis was expressed, for instance, by @nuijten2016prevalence who studied the prevalence of statistical reporting errors in articles published in different areas of psychological science. @nuijten2016prevalence hypothesized that articles published in social psychology journals would have higher error rates than articles published in other psychological journals while not expressing expectations about the error rate distribution among the other journals. Here again it is not possible to apply standard tests, since we cannot derive fixed proportions based on the hypothesis. Generally, if researchers and practitioners can utilize statistical methods for testing informed hypotheses, they are able to test hypotheses that relate more closely to their theories.

In the Bayesian framework, researchers can compare models that instantiate the hypotheses of interest by means of Bayes factors [@jeffreys1935some; @kass1995bayes]. To compute Bayes factors for informed hypotheses several \texttt{R} packages are already available. For instance, with the package \textbf{multinomineq} [@heck2019multinomial] users can specify inequality constrained hypotheses but also more general linear inequality constraints for multinomial models as well as models that feature independent binomials. The \textbf{BAIN} package [@gu2019bain] allows for the evaluation of inequality constraints in structural equation models. The package \textbf{BFpack} [@bfpack] evaluates informed hypotheses for statistical models such as univariate and multivariate normal linear models, generalized linear models, special cases of linear mixed models, survival models, and relational event models. Outside of \texttt{R}, the Fortran 90 program \textbf{BIEMS} [@mulder2012biems] allows for the evaluation of order constraints for multivariate linear models such as MANOVA, repeated measures, and multivariate regression. All these packages rely on one of two methods to approximate order constrained Bayes factors: the encompassing prior approach [@gu2014bayesian; @klugkist2005bayesian; @hoijtink2008bayesian; @hoijtink2011informative] and the conditioning method [@mulder2009bayesian; @mulder2014prior; @mulder2016bayes]. Even though these methods are currently widely used, they are known to become increasingly unreliable and inefficient as the number of constraints increases or the parameter space of the constrained model decreases [@sarafoglou2020evaluatingPreprint]. 

In contrast to these available packages, \textbf{multibridge} uses a bridge sampling routine that enables users to compute Bayes factors for informed hypotheses more reliably and efficiently [@bennett1976efficient; @meng1996simulating; @sarafoglou2020evaluatingPreprint]. The workhorse for this analysis, the bridge sampling algorithm, constitutes a special case of the algorithm implemented in the \texttt{R} package \textbf{bridgesampling} [@gronau2017bridgesampling]. The \textbf{bridgesampling} package, allows users to estimate the marginal likelihood for a wide variety of models, including models implemented in Stan [@stan2020]. However, the algorithm implemented in \textbf{bridgesampling} is not suitable for models that include constraints on probability vectors and hence is unsuitable for the analysis of categorical data. Therefore, in \textbf{multibridge}, we tailored the bridge sampling algorithm such that it accommodates the specification of informed hypotheses on probability vectors. The package then produces an estimate for the Bayes factor in favor of or against the informed hypothesis. The resulting Bayes factor compares the evidence for the informed hypotheses to the encompassing hypothesis that imposes no constraints on the underlying category proportions. Alternatively, the informed hypothesis can be tested against the null hypothesis that all underlying category proportions are exactly equal. Given this result, users can then either receive a visualization of the posterior parameter estimates under the encompassing hypothesis using the \texttt{plot}-method, or get more detailed information on how the Bayes factor is composed using the \texttt{summary}-method. For hypotheses that include mixtures between equality and inequality constrained hypotheses the \texttt{bayes\_factor} method shows the conditional Bayes factor for the inequality constraints given the equality constraints and a Bayes factor for the equality constraints. The general workflow of \textbf{multibridge} is illustrated in Figure \@ref(fig:scheme-multibridge). Table \ref{table:s3_methods} summarizes all S3 methods currently available in \textbf{multibridge}.

(ref:scheme-multibridge-caption) The \textbf{multibridge} workflow. The user specifies the data values (\texttt{x} and \texttt{n} for binomial models and \texttt{x} for multinomial models, respectively), the informed hypothesis (\texttt{Hr}), the $\alpha$ and $\beta$ parameters of the Binomial prior distributions (\texttt{a} and \texttt{b}) or the concentration parameters for the Dirichlet prior distribution (\texttt{a}), respectively, and the category labels of the factor levels (\texttt{factor\_levels}). The functions \texttt{mult\_bf\_informed} and \texttt{binom\_bf\_informed} then produce an estimate for the Bayes factor of the informed hypothesis versus the encompassing or the null hypothesis. Based on these results different S3 methods can be used to get more detailed information on the individual components of the analysis (e.g., \texttt{summary}, \texttt{bayes\_factor}), and parameter estimates of the encompassing distribution (\texttt{plot}).

```{r scheme-multibridge, fig.cap='(ref:scheme-multibridge-caption)', out.width = "400px", message=FALSE}
knitr::include_graphics("scheme_multibridge.png", auto_pdf = TRUE)
``` 

\begin{table}
\caption {S3 methods available in $\textbf{multibridge}$}
\label{table:s3_methods}
\begin{center}
\begin{tabular}{p{4cm}p{3.5cm}p{9cm}}
        \toprule
Function Name(s) & S3 Method & Description \\\midrule
$\texttt{mult\_bf\_informed}$, $\texttt{binom\_bf\_informed}$ & $\texttt{print}$ & Prints model specifications and descriptives. \\
 & $\texttt{summary}$ &  Prints and returns the Bayes factor and associated hypotheses for the full model, and all equality and inequality constraints.\\
  & $\texttt{plot}$ &  Plots the posterior median and 95\% credible interval of the parameter estimates of the encompassing model.\\
 & $\texttt{bayes\_factor}$ & Contains all Bayes factors and log marginal likelihood estimates for inequality constraints.\\
 & $\texttt{samples}$ & Extracts prior and posterior samples from constrained distribution (if bridge sampling was applied). \\
& $\texttt{bridge\_output}$    &  Extracts bridge sampling output and associated error measures.\\
& $\texttt{restriction\_list}$ & Extracts restriction list and associated informed hypothesis. \\
$\texttt{mult\_bf\_inequality}$, $\texttt{binom\_bf\_inequality}$  & $\texttt{print}$ & Prints the bridge sampling estimate for the log marginal likelihood and the corresponding percentage error. \\
& $\texttt{summary}$ & Prints and returns the bridge sampling estimate for the log marginal likelihood and associated error terms.\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

<!-- The following signs can be used to encode informed hypotheses: "\texttt{<}" and "\texttt{>}" for inequality constraints, "\texttt{=}" for equality constraints, "\texttt{,}" for free parameters, and "\texttt{\&}" for independent hypotheses. The restricted hypothesis can either be a string or a character vector. For instance, the hypothesis \texttt{c("theta1 < theta2, theta3")} means, that (1) $\theta_1$ is smaller than both \texttt{theta_2} and \texttt{theta_3}, and (2) \texttt{theta_2} and \texttt{theta_3} are both bigger than \texttt{theta_1}, but are not influenced by each other. The hypothesis \texttt{c("theta1 < theta2 = theta3 \& theta4 > theta5")} means that (1) two independent hypotheses are stipulated: "\texttt{theta1 < theta2 = theta3}" and "\texttt{theta4 > theta5}", (2) the restrictions on the parameters \texttt{theta_1}, \texttt{theta_2}, and \texttt{theta_3} do not influence the restrictions on the parameters \texttt{theta_4} and \texttt{theta_5}, (3) \texttt{theta_1}is smaller than \texttt{theta_2} and \texttt{theta_3}, (4) \texttt{theta_2} and \texttt{theta_3} are assumed to be equal, and (5) \texttt{theta_4} is larger than \texttt{theta_5}. -->

The remainder of this article is organized as follows: In the methods section, we describe the Bayes factor identity for informed hypotheses in binomial and multinomial models, and present the bridge sampling routine implemented in the \textbf{multibridge} package including details of the necessary transformations required for this routine. In Section 3, we will schematically introduce the most relevant functions in \textbf{multibridge} and their arguments. Section 4 illustrates how to use the \textbf{multibridge} package to estimate parameters, and compute Bayes factors using two examples.

# Methods

In this section we formalize multinomial models and models that feature independent binomial probabilities as we have implemented them in \textbf{multibridge}. In the multinomial model, we assume that the vector of observations \textbf{x} in the $K$ categories follow a multinomial distribution in which the parameters of interest, $\boldsymbol{\theta}$, represent the underlying category proportions. Since we assume a dependence between the $K$ categories, the vector of probability parameters is sum-to-one constrained, such that $\sum_{k = 1}^K (\theta_1, \cdots, \theta_K) = 1$. Therefore, a suitable choice for a prior distribution for $\boldsymbol{\theta}$ is the Dirichlet distribution with concentration parameters $\boldsymbol{\alpha}$:

\begin{align}
  x_1, \cdots, x_K &\sim \text{Multinomial}(\sum_{k = 1}^K x_k, \theta_1, \cdots, \theta_K) \\
  \theta_1, \cdots, \theta_K &\sim \text{Dirichlet}(\alpha_1, \cdots, \alpha_K),
\end{align}

where $\boldsymbol{\alpha}$ can be interpreted as vector of *a priori* category counts. Since the multinomial model constitutes a generalization of the binomial model (for $K \geq 2$), the formalization of a model that features independent binomial probabilities is very similar. In the binomial model, we assume that the elements in the vector of successes \textbf{x} and the elements in the vector of total number of observations \textbf{n} in the $K$ categories follow independent binomial distributions. As in the multinomial model, the parameter vector of the binomial success probabilities $\boldsymbol{\theta}$ contains the underlying category proportions, however, in this model we assume that categories are independent which removes the sum-to-one constraint. Therefore, a suitable choice for a prior distribution for $\boldsymbol{\theta}$ is a vector of independent beta distributions with parameters $\boldsymbol{\alpha}$ and $\boldsymbol{\beta}$:

\begin{align}
  x_1 \cdots x_K & \sim \prod_{k = 1}^K \text{Binomial}(\theta_k, n_k) \\
  \theta_1 \cdots \theta_K &\sim \prod_{k = 1}^K \text{Beta}(\alpha_k, \beta_k),
\end{align}
where $\boldsymbol{\alpha}$ can be interpreted as vector of *a priori* successes that observations fall within the various categories and $\boldsymbol{\beta}$ can be interpreted as vector of *a priori* failures.

## Bayes factor

With \textbf{multibridge} package, it is possible to collect evidence for informed hypotheses on a parameter vector $\boldsymbol{\theta}$ by means of the Bayes factor. Bayes factors compare the relative evidence of two hypotheses in the light of the data. It is defined as the ratio of marginal likelihoods of the respective hypotheses. For instance, the Bayes factor for the informed hypothesis versus a hypothesis that lets all parameters free to vary is defined as:

\begin{align*}
\text{BF}_{re} = \cfrac{\overbrace{p(\mathbf{x}\mid \mathcal{H}_r)}^{\substack{\text{Marginal likelihood}\\\text{under $\mathcal{H}_r$}}}}{\underbrace{p(\mathbf{x}\mid \mathcal{H}_e)}_{\substack{\text{Marginal likelihood}\\\text{under $\mathcal{H}_e$}}}},
\end{align*}

where the subscript $r$ denotes the informed (restricted) hypothesis and $e$ denotes the (encompassing) hypothesis which predicts that all parameters free to vary. In \textbf{multibridge} we use two different methods to compute Bayes factors, one method evaluates hypotheses that feature equality constraints on $\boldsymbol{\theta}$ and one method evaluates hypotheses that feature inequality constraints on $\boldsymbol{\theta}$. Both methods will be outlined below. In cases where informed hypotheses feature mixtures between inequality and equality constraints, we compute the corresponding Bayes factor $\text{BF}_{re}$ by multiplying the individual Bayes factors for both constrait types with each other:

$$
\text{BF}_{re}
= \text{BF}_{1e} \times \text{BF}_{2e} \mid \text{BF}_{1e},
$$
where the subscript $1$ denotes the hypothesis that only features equality constraints and the subscript $2$ denotes the hypothesis that only features inequality constraints. A Bayes factor for mixtures thus factors into a Bayes factor for the equality constraints, $\text{BF}_{1e}$, and  a conditional Bayes factor for the inequality constraints given the equality constraints $\text{BF}_{2e} \mid \text{BF}_{1e}$ [for the proof, see @sarafoglou2020evaluatingPreprint].

## The Bayes Factor For Equality Constraints

The Bayes factor for the equality constraints can be computed analytically both for binomial and multinomial models. For binomial models, the function \texttt{binom\_bf\_equality} is available to compute $\text{BF}_{0e}$. \Frederik{I'm a little confused by the notation here. Above BF1e were equality constraints.} Assuming that the first $i$ binomial probabilities in a model are equality constrained, the Bayes factor is defined as:
\begin{align*}
\text{BF}_{0e}
&= \cfrac{\prod_{i < k} \text{B}(\alpha_i\text{, } \beta_i)}{\prod_{i < k} \text{B}(\alpha_i + x_i\text{, } \beta_i + n_i - x_i)} \times \cfrac{\text{B}(\alpha_+ + x_+ - i + 1\text{, } \beta_+ + n_+ - x_+ - i + 1)}{\text{B}(\alpha_+ - i + 1\text{, }\beta_+ - i + 1)}
\end{align*}
where $\text{B}()$ denotes the beta function and $\alpha_+ = \sum_{i<k}\alpha_i$, $\beta_+ = \sum_{i<k}\beta_i$, $x_+ = \sum_{i<k} x_i$ and $n_+ = \sum_{i<k} n_i$. The latter factor introduces a correction for marginalizing which stems from the change in degrees of freedom, when we collapse $i$ equality constraint parameters: For $i$ collapsed categories, $i - 1$ degrees of freedom are lost which are subtracted from the prior parameters in the corresponding Binomial distribution. 

For multinomial models, the function \texttt{multBayes\_bf\_equality} is available. Assuming again that the first $i$ category probabilities in a model are equality constraint, the Bayes factor $\text{BF}_{0e}$ is defined as:
\begin{align*}
\text{BF}_{0e} = \frac{\text{B}(\boldsymbol{\alpha}+\mathbf{x})}{\text{B}(\boldsymbol{\alpha})} \, \left(\frac{1}{i}\right)^{\sum_{i<k} x_i}\,\frac{
 \text{B}\left(\sum_{i<k}\alpha_i - i + 1\text{, }\alpha_{k}\text{, }\dots\text{, }\alpha_K\right)}{\text{B}\left(\sum_{i<k}\alpha_i+x_i - i + 1\text{, }\alpha_{k}+x_{k}\text{, }\dots\text{, }\alpha_K+x_K\right)}.
\end{align*}

## The Bayes Factor For Inequality Constraints

To approximate the Bayes factor for informed hypotheses, @klugkist2005bayesian derived the following identity:

\begin{align}
\label{Eq:klugkistIdentity}
\text{BF}_{re} &= \cfrac{\overbrace{p(\boldsymbol{\theta} \in \mathcal{R}_r \mid \mathbf{x}\text{, }\mathcal{H}_e)}^{\substack{\text{Proportion of posterior parameter}\\\text{space consistent with the restriction}}}}{\underbrace{p(\boldsymbol{\theta} \in \mathcal{R}_r \mid  \mathcal{H}_e)}_{\substack{\text{Proportion of prior parameter}\\\text{space consistent with the restriction}}}}.
\end{align}

Recently, @sarafoglou2020evaluatingPreprint showed that the Bayes factor $\text{BF}_{re}$ can also be interpreted as ratio of two marginal likelihoods:

\begin{align}
\label{Eq:sarafoglouIdentity}
\text{BF}_{re} =
\cfrac{\overbrace{p(\boldsymbol{\theta} \in \mathcal{R}_r \mid \mathbf{x}\text{, }\mathcal{H}_e)}^{\substack{\text{Marginal likelihood of}\\\text{posterior distribution}}}}{\underbrace{p(\boldsymbol{\theta} \in \mathcal{R}_r \mid  \mathcal{H}_e)}_{\substack{\text{Marginal likelihood of}\\\text{prior distribution}}}}.
\end{align}

\Frederik{Hmm, maybe I'm missing something, but given that the two equations appear to be the same, wouldn't it suffice to omit the second equation and just offer the following reinterpretation of the terms in the text?}

In this identity, $p(\boldsymbol{\theta} \in \mathcal{R}_r \mid \mathbf{x}\text{, }\mathcal{H}_e)$ denotes the marginal likelihood of the constrained posterior distribution and $p(\boldsymbol{\theta} \in \mathcal{R}_r \mid  \mathcal{H}_e)$ denotes the marginal likelihood of the constrained prior distribution. Even though both identities are mathematically equivalent, the methods to estimate these identities differ substantially. 
In the first case, the number of samples from the encompassing distribution in accordance with the inequality constrained hypothesis serve as an estimate for the proportion of prior parameter space consistent with the restriction. Although easy to implement, this definition implies that the accuracy of this estimate is strongly dependent on the number of the constrained parameters in the model and the size of the constrained parameter space. That is, as the constraints become stronger, the constrained parameter space decreases. As a result it becomes less likely that draws from the encompassing distribution will fall into the constrained region, so that in some cases the estimation of the Bayes factor becomes practically impossible [@sarafoglou2020evaluatingPreprint].

However, when we interpret the Bayes factor $\text{BF}_{re}$ as ratio of marginal likelihoods and we are able to sample from the constrained prior and posterior distributions, we can utilize numerical sampling methods such as bridge sampling to obtain the estimates. Crucially, in this approach, it does not matter how small the constrained parameter space is in proportion to the encompassing density. This gives the method a decisive advantage over the encompassing prior approach in terms of accuracy and efficiency especially (1) when binomial and multinomial models with relatively high number of categories (i.e., $K > 10$) are evaluated and (2) when relatively little posterior mass falls in the constrained parameter space.

<!-- ### PUT THIS LATER -->
<!-- The conditional Bayes factor of inequality constraints given the equality constraints involves expectations over the conditional Binomial distributions -->
<!-- $$ -->
<!-- p(\boldsymbol{\theta}_r \mid \boldsymbol{\theta}_0 \in\mathcal{R}_0\text{, }\mathcal{H}_e) =  \text{Binomial}\left(\sum_{i<k}\alpha_k-j+1\text{, }\sum_{i<k}\beta_k-j+1\right) \times \prod_{k=j + 1}^K -->
<!-- \text{Binomial}\left(\alpha_{k}\dots\text{, }\beta_{k}\right) -->
<!-- $$ -->
<!-- and -->
<!-- $$ -->
<!-- p(\boldsymbol{\theta}_r \mid \boldsymbol{\theta}_0 \in \mathcal{R}_0\text{, }\mathbf{x}\text{, }\text{, }\mathbf{n}\text{, }\mathcal{H}_e) = \text{Binomial}\left(\sum_{i<k}(\alpha_k + x_k) -j+1 \text{, }\sum_{i<k} (\beta_k + n_k - x_k) -j+1 \right) \times \prod_{k=j + 1}^K -->
<!-- \text{Binomial}\left(\alpha_{k}\dots\text{, }\beta_{k}\right), -->
<!-- $$ -->
<!-- whose marginal likelihoods are approximated using bridge sampling. -->
<!-- The conditional Bayes factor of inequality constraints given the equality constraints involves expectations over the conditional Dirichlet distributions -->
<!-- $$ -->
<!-- p(\boldsymbol{\theta}_r \mid \boldsymbol{\theta}_0 \in\mathcal{R}_0\text{, }\mathcal{H}_e) = \text{Dirichlet}\left(\sum_{i<k}\alpha_k-j+1\text{, }\alpha_{j+1}\dots\text{, }\alpha_K\right) -->
<!-- $$ -->
<!-- and -->
<!-- $$ -->
<!-- p(\boldsymbol{\theta}_r \mid \boldsymbol{\theta}_0 \in \mathcal{R}_0\text{, }\mathbf{x}\text{, }\mathcal{H}_e) = \text{Dirichlet}\left(\sum_{i<k}(\alpha_k+x_k)-j+1\text{, }\alpha_{j+1}+x_{j+1}\dots\text{, }\alpha_K+x_K\right). -->
<!-- $$ -->


## The Bridge Sampling Method

Bridge sampling is a method to estimate the ratio of two marginal likelihoods [@bennett1976efficient; @meng1996simulating]. In \textbf{multibridge}, we are using bridge sampling to estimate the identity presented in Equation \ref{Eq:sarafoglouIdentity}. But instead of estimating the ratio of marginal likelihoods directly, we implemented a version of bridge sampling that estimates one marginal likelihood at the time. This approach has the benefit that it increases the accuracy of the method without considerably increasing its computational efficiency [@overstall2010default]. Specifically, we subsequently estimate the marginal likelihood for the constrained prior distribution and the marginal likelihood of the constrained posterior distribution.

When applying this modified version of the bridge sampling method, we estimate each marginal likelihood by means of a so-called proposal distribution. In \textbf{multibridge} this proposal distribution is the multivariate normal distribution. To estimate the marginal likelihood, bridge sampling only requires samples from the distribution of interest---the so-called target distribution---and samples from the proposal distribution.

Samples from the target distribution---that is the constrained prior and posterior Dirichlet distribution for multinomial models and constrained prior and posterior beta distributions for binomial models---are drawn through the Gibbs sampling algorithms proposed by @damien2001sampling. For binomial models, we apply the suggested Gibbs sampling algorithm for constrained beta distributions. In the case of the multinomial models, we apply an algorithm that simulates values from constrained Gamma distributions which are then transformed into Dirichlet random variables [for details, see Appendix C in @sarafoglou2020evaluatingPreprint]. To sample efficiently from these distributions, \textbf{multibridge} provides a \texttt{C++} implementation of this algorithm.

Samples from the proposal distribution can be generated using the standard \texttt{rmvnorm}-function from the \texttt{R} package \textbf{stats}. \Frederik{\textbf{mvtnorm}?} The vector of means and the covariance matrix of this distribution are derived from one part of the samples of the probit transformed target distribution. The reason for this approach is that the efficiency of the bridge sampling method is optimal only if the target and proposal distribution operate on the same parameter space and have sufficient overlap. We therefore probit transform the samples of the constrained distributions to move the samples from the probability space to the entire real line. Subsequently, we use half of these draws to construct the proposal distribution using the method of moments. Details on the probit transformations are provided in the appendix. Thus, for the marginal likelihood of the constrained prior distribution, the modified bridge sampling identity is then defined as

\begin{align}
    p(\boldsymbol{\theta} \in \mathcal{R}_r \mid \mathcal{H}_e) = \cfrac{\mathbb{E}_{g(\boldsymbol{\theta})}\left(p(\boldsymbol{\theta}\mid \mathcal{H}_e) \mathbb{I}(\boldsymbol{\theta}\in\mathcal{R}_r)h(\boldsymbol{\theta})\right)}{\mathbb{E}_{\text{prior}} \left(g(\boldsymbol{\theta})h(\boldsymbol{\theta})\right)},
    \label{Eq:bridgeidentity}
\end{align}
where the term $h(\boldsymbol{\theta})$ refers to the bridge function proposed by @meng1996simulating and $g(\boldsymbol{\theta})$ refers to the proposal distribution. The numerator evaluates the unnormalized density for the constrained prior distribution with samples from the proposal distribution. The denominator evaluates the normalized proposal distribution with samples from the constrained prior distribution. Using this identity, we receive the bridge sampling estimator for the marginal likelihood of the constrained prior distribution by applying the iterative scheme proposed by @meng1996simulating:

\begin{align*}
    \hat p(\boldsymbol{\theta} \in \mathcal{R}_r \mid \mathcal{H}_e)^{(t+1)} \approx \cfrac{\cfrac{1}{N_2} \sum_{m = 1}^{N_2} \cfrac{\ell_{2,m}}{s_1 \ell_{2,m} + s_2 p(\boldsymbol{\tilde \theta_m} \in \mathcal{R}_r \mid \mathcal{H}_e)^{(t)}}}
    {\cfrac{1}{N_1} \sum_{n = 1}^{N_1} \cfrac{1}{s_1 \ell_{1,n} + s_2 p(\boldsymbol{\theta^*_n} \in \mathcal{R}_r \mid \mathcal{H}_e)^{(t)}}},
    %\label{Eq:bridgeIterativeScheme}
\end{align*}
where $N_1$ denotes the number of samples drawn from the constrained distribution, that is, $\boldsymbol{\theta}^* \sim p(\boldsymbol{\theta} \mid \mathcal{H}_r)$, $N_2$ denotes the number of samples drawn from the proposal distribution, that is $\boldsymbol{\tilde \theta} \sim g(\boldsymbol{\theta})$,
$s_1 = \frac{N_1}{N_2 + N_1}$, and $s_2 = \frac{N_2}{N_2 + N_1}$. The quantities $\ell_{1,n}$ and $\ell_{2,m}$ are defined as follows:

\begin{align}
    \ell_{1,n} &= \cfrac{q_{1,1}}{q_{1,2}}  = \cfrac{p(\boldsymbol{\theta^*_n}\mid \mathcal{H}_e) \mathbb{I}(\boldsymbol{\theta^*_n}\in\mathcal{R}_r)}{g(\boldsymbol{\xi_n}^*)},\\
    \ell_{2,m} &= \cfrac{q_{2,1}}{q_{2,2}} = \cfrac{p(\boldsymbol{\tilde \theta_m}\mid \mathcal{H}_e) \mathbb{I}(\boldsymbol{\tilde \theta_m}\in\mathcal{R}_r)}{g(\boldsymbol{\tilde \xi_m})},
\end{align}
where $\boldsymbol{\xi_n}^* = \Phi^{-1}\left(\cfrac{\boldsymbol{\theta^*_n} - \mathbf{l}}{\mathbf{u} - \mathbf{l}}\right)$, and $\boldsymbol{\tilde \theta_m} = ((\mathbf{u} - \mathbf{l})\Phi(\boldsymbol{\tilde \xi_m}) + \mathbf{l}) \left|J\right|)$. The quantity $q_{1,1}$ refers to the evaluations of the constrained distribution for constrained samples and $q_{1,2}$ refers to the proposal evaluations for constrained samples, respectively. The quantities $q_{2,1}$ refers to evaluations of the constrained distribution for samples from the proposal and $q_{2,2}$ refers to the proposal evaluations for samples from the proposal, respectively. Note that the quantities $\ell_{1,n}$ and $\ell_{2,m}$ have been adjusted to account for the necessary parameter transformations to create overlap between the constrained distributions and the proposal distribution. \textbf{multibridge} runs the iterative scheme until the tolerance criterion suggested by @gronau2017tutorial is reached, that is: 
\begin{align*}
\cfrac{\mid \hat p(\boldsymbol{\theta} \in \mathcal{R}_r \mid \mathcal{H}_e)^{(t + 1)} - \hat p(\boldsymbol{\theta} \in \mathcal{R}_r \mid \mathcal{H}_e)^{(t)} \mid}{\hat p(\boldsymbol{\theta} \in \mathcal{R}_r \mid \mathcal{H}_e)^{(t + 1)}} &\leq 10^{-10}.
\end{align*}

The bridge sampling estimate for the log marginal likelihood of the constrained distribution and its associate relative mean square error, the number of iterations, and the quantities $q_{1,2}$, $q_{1,2}$, $q_{1,2}$, and $q_{1,2}$ are included in the standard output in \textbf{multibridge}. The function to compute the relative mean square error was taken from the R package \textbf{bridgesampling}.\Frederik{Is this important enough to mention it here?}\Alex{Not sure where to include it otherwise}

# Usage and Examples


The \textbf{multibridge} package can be installed from the Comprehensive R Archive Network (CRAN) at
\url{https://CRAN.R-project.org/package=multibridge}:

```{r, echo = TRUE, eval = FALSE}
install.packages('multibridge')
library('multibridge')
```

```{r, echo = FALSE}
library('multibridge')
```

A list of all currently available functions and datasets is given in Table \ref{table:core_functions}. Additional examples are available as vignettes (see \url{https://cran.r-project.org/package=multibridge}, or `vignette(package = "multibridge")`). The two core functions of \textbf{multibridge}---the \texttt{mult\_bf\_informed}-function and the \texttt{binom\_bf\_informed}-function---can be illustrated schematically as follows:

```{r, eval=FALSE, echo=TRUE}
mult_bf_informed(x, Hr, a factor_levels)
binom_bf_informed(x, n, Hr, a, b, factor_levels)
```

The basic required arguments for these functions are listed in Table \ref{tab:arguments}. In the following, we will outline two examples on how to use \textbf{multibridge} to compare an informed hypothesis to a null or encompassing hypothesis. In addition, the first example shows how two informed hypotheses can be compared to each other.

\begin{table}
\caption{To estimate the Bayes factor in favor for or against the specified informed hypothesis, the user provides the core functions \texttt{mult\_bf\_informed} and \texttt{binom\_bf\_informed} with the following basic required arguments}
\label{tab:arguments}
\begin{center}
\begin{tabular}{p{4cm}p{12cm}}
        \toprule
Argument & Description \\\midrule
\texttt{x} & numeric. a vector with data (for multinomial models) or a vector of counts of successes, or a two-dimensional table (or matrix) with 2 columns, giving the counts of successes and failures, respectively (for binomial models)  \\
\texttt{n} &  numeric. Vector of counts of trials. Must be the same length as \texttt{x}. Ignored if \texttt{x} is a matrix or a table \\
\texttt{Hr} & string or character. Encodes the user specified informed hypothesis. Users can either use the specified \texttt{factor\_levels} or numerical indeces to refer to parameters.\\
\texttt{a} & numeric. Vector with concentration parameters of Dirichlet distribution (for multinomial models) or $\alpha$ parameters for independent beta distributions (for binomial models). Default sets all parameters to 1 \Frederik{Must be the same length as \texttt{x}?} \\
\texttt{b} & numeric. Vector with $\beta$ parameters. Must be the same length as \texttt{x}. Default sets all $\beta$ parameters to 1 \\
\texttt{factor\_levels} &  character. Vector with category labels. Must be the same length as \texttt{x}\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\begin{table}
\caption {Core functions available in $\textbf{multibridge}$}
\label{table:core_functions}
\begin{center}
\begin{tabular}{p{5.5cm}p{10.5cm}}
        \toprule
Function Name(s) & Description \\\midrule
$\texttt{mult\_bf\_informed}$ & Evaluates informed hypotheses on multinomial parameters.  \\
$\texttt{mult\_bf\_inequality}$ & Estimates the marginal likelihood of a constrained prior or posterior Dirichlet distribution.  \\
$\texttt{mult\_bf\_equality}$ & Computes Bayes factor for equality constrained multinomial parameters using the standard Bayesian multinomial test.  \\
$\texttt{mult\_tsampling}$ & Samples from truncated prior or posterior Dirichlet density.\\
$ \texttt{lifestresses}, \texttt{peas}$ & Datasets associated with informed hypotheses in multinomial models.\\\midrule
$\texttt{binom\_bf\_informed}$ & Evaluates informed hypotheses on binomial parameters.  \\
$\texttt{binom\_bf\_inequality}$ & Estimates the marginal likelihood of constrained prior or posterior beta distributions.\\
$\texttt{binom\_bf\_equality}$ & Computes Bayes factor for equality constrained binomial parameters.  \\
$\texttt{binom\_tsampling}$ & Samples from truncated prior or posterior beta densities.\\
$ \texttt{journals}$ & Dataset associated with informed hypotheses in binomial models.\\\midrule
$ \texttt{generate\_restriction\_list}$ & Encodes the informed hypothesis.\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

## Example 1: Applying A Benford Test to Greek Fiscal Data

\Frederik{Should we maybe refer to it as Newcomb-Benford’s Law?}

The first digit phenomenon, otherwise known as Benford's law [@benford1938law; @newcomb1881note] states that the expected proportion of leading digits in empirical data can be formalized as follows: for any given leading digit $d, d = (1, \cdots, 9)$ the expected proportion is approximately equal to $$\mathbb{E}_{\theta_d}= \text{log}_{10}((d + 1)/d).$$ This means that in an empirical dataset numbers with smaller leading digits are more common than numbers with larger leading digits. Specifically, a number has leading digit $1$ in $30.1 \%$ of the cases, and leading digit $2$ in $17.61 \%$ of the cases; leading digit $9$ is the least frequent digit with an expected proportion of only $4.58 \%$ (see Table \ref{Tab:benford} for an overview of the expected proportions). Examples of empirical data for which this relationship holds include data on population sizes, death rates, baseball statistics, atomic weights of elements, and physical constants [@benford1938law]. In contrast, generated data, such as telephone numbers, do in general not obey Benford's law [@hill1995statistical]. Given that Benford's law applies to empirical data but not artificially generated data, a so-called Benford test can be used to check whether a set of data obey Benford's law and therefore exhibit an important property of empirical datasets. Benford's tests are used in fields like accounting and auditing to check for indications for poor data quality, for instance, in fiscal statements [for an overview, see e.g., @durtschi2004effective; @nigrini1997use; @nigrini2012benford]. Data that do not pass the Benford test, should raise audit risk concerns, meaning that it is recommended that the data undergo additional follow-up checks [@nigrini2019patterns].

In the following, we discuss three possible Bayesian adaptations of the Benford's test. In a first scenario we simply conduct Bayesian multinomial test in which we test the point-null hypothesis $\mathcal{H}_0$ which predicts a Benford distribution against the encompassing hypothesis $\mathcal{H}_{e}$ which leaves all proportions of first digits free to vary. Testing against the encompassing hypothesis is considered standard practice, yet, it leads to an unfair comparison to the detriment of the null hypothesis. In general, if we are dealing with a high-dimensional parameter space and the competing hypotheses differ largely in their complexity, the Bayes factor generally favors the less complex hypothesis (i.e., $\mathcal{H}_{e}$) even if the data follow the predicted trend of the more complex hypothesis considerably well. In a second scenario we therefore test the null hypothesis against an alternative hypothesis, denoted as $\mathcal{H}_{r1}$, which predicts a monotonically decreasing trend in the proportions of leading digits. The hypothesis $\mathcal{H}_{r1}$ exerts considerably more constraints than $\mathcal{H}_{e}$ and provides a more sensitive to test if our primary goal is to test whether data comply with Benford's law or whether the data follow a similar but different trend. In a third scenario, where the main goal is to identify fabricated data, we could test the null hypothesis against a hypothesis, which predicts a trend that is characteristic for manipulated data. This hypothesis, which we denote as $\mathcal{H}_{r2}$, could be derived from empirical research on fraud or be based on observed patterns from former fraud cases. For instance, @hill1988random instructed students to produce a series of random numbers; in the resulting data the proportion of the leading digit $1$ occurred most often and the digits $8$ and $9$ occurred least often which is consistent with the general pattern of Benford's law. However, the proportion for the remaining leading digits were approximately equal. We do want to note that the predicted distribution derived from @hill1988random is not currently used as a test to detect fraud. However, for the sake of simplicity, if we assume that this pattern could be an indication of fabricated auditing data, the Bayes factor would quantify the evidence of whether the proportion of first digits resemble authentic or fabricated data.

### Data and Hypothesis

<!-- # ```{r, echo = FALSE} -->
<!-- # dat0        <- read.table('EUAuditing.csv', header=TRUE, sep=',') -->
<!-- # greece_dat  <- dat0[1:9, grepl('Greece_', colnames(dat0))] -->
<!-- # greece_N    <- unlist(dat0[10, grepl('Greece_', colnames(dat0))]) -->
<!-- # for(i in 1:ncol(greece_dat)){ -->
<!-- #   greece_dat[, i] <- greece_dat[, i] * greece_N[i] -->
<!-- # } -->
<!-- # # round numbers to receive integers -->
<!-- # greece_dat$all <- round(rowSums(greece_dat)) -->
<!-- # dat <- data.frame(leading_digit = 1:9, -->
<!-- #                   benford_proportions = dat0$Benford[1:9], -->
<!-- #                   observed_counts = greece_dat$all, -->
<!-- #                   observed_proportions = greece_dat$all/sum(greece_dat$all)) -->
<!-- # ``` -->

The data we use to illustrate the computation of Bayes factors were originally published by the European statistics agency "Eurostat" and served as basis for reviewing the adherence to the Stability and Growth Pact of EU member states. @rauch2011fact conducted a Benford test on data related to budget deficit criteria, that is, public deficit, public dept and gross national products. The data used for this example feautres the proportion of first digits from fiscal data from Greece in the years between $1999$ and $2010$; a total of $N= 1{,}497$ numerical data were included in the analysis. We choose this data, since the Greek government deficit and debt statistics states has been repeatedly criticized by the European Commission in this timespan [@europeanCommision2004; @europeanCommision2010]. In particular, the commission has accused the Greek statistical authorities to have misreported deficit and debt statistics. For further details on the dataset see @rauch2011fact. The observed proportions are displayed in Table \ref{Tab:benford}, the figure displaying the observed versus the expected proportions are displayed in Figure \ref{fig:benford}.

\begin{table}[h]
	\centering
	\caption{The Table shows the Observed Counts, Observed Proportions, and Expected Proportions of first digits in Greece governmental data. The total sample size was $N = 1{,}497$ observations. Note that the observed proportions and counts deviate slightly from those reported in Rauch et al. (2011) (probably due to rounding errors).}
	\begin{tabular}{cccp{4cm}}
		\hline
Leading digit & Observed Counts & Observed Proportions & Expected Proportions: Benford's Law  \\
		\hline
		1 & 509 & 0.340 & 0.301  \\
		2 & 353 & 0.236 & 0.176  \\
		3 & 177 & 0.118 & 0.125  \\
		4 & 114 & 0.076 & 0.097  \\
		5 & 77 & 0.051 & 0.079  \\
		6 & 77 & 0.051 & 0.067  \\
		7 & 53 & 0.035 & 0.058  \\
		8 & 73 & 0.049 & 0.051  \\
		9 & 64 & 0.043 & 0.046  \\
		\hline
	\end{tabular}
    \label{Tab:benford}
\end{table}

In this example, the parameter vector of the multinomial model, $\theta_1, \cdots, \theta_K$, reflects the probabilities of a leading digit in the Greek fiscal data being a number from $1$ to $9$.  Thus, we can formalize the discussed hypotheses as follows. The null hypothesis specifies that the proportions of first digits obeys Benford's law:
$$\mathcal{H}_0 : \boldsymbol{\theta}_0 = (0.301, 0.176, 0.125, 0.097, 0.079, 0.067, 0.058, 0.051, 0.046).$$

We are testing the null hypothesis against the following alternative hypotheses:
\begin{align*}
\mathcal{H}_e &: \boldsymbol{\theta} \sim \text{Dirichlet}(\boldsymbol{\alpha}), \\
\mathcal{H}_{r1} &: \theta_1 > \theta_2 > \theta_3 > \theta_4 > \theta_5 > \theta_6 > \theta_7 > \theta_8 > \theta_9, \\
\mathcal{H}_{r2} &:  \theta_1 > (\theta_2 = \theta_3 = \theta_4 = \theta_5 = \theta_6 = \theta_7) > (\theta_8, \ \theta_9).
\end{align*}

In cases, in which we are interested in computing two informed hypotheses with each other, we need to make use of the transitivity property of the Bayes factor. For instance, if we would like to compare the two informed hypotheses $\mathcal{H}_{r1}$ and $\mathcal{H}_{r2}$ with each other, we would first compute $\text{BF}_{er1}$ and $\text{BF}_{er2}$ and then yield $\text{BF}_{r1r2}$ as follows:
$$\text{BF}_{r1e} \times \text{BF}_{er2} = \text{BF}_{r1r2}.$$

### Method
We can compare $\mathcal{H}_0$ and $\mathcal{H}_e$ by means of a Bayesian multinomial test, that is, we stipulate equality constraints on the entire parameter vector $\boldsymbol{\theta}$. The corresponding Bayes factor is thus computationally straightforward; we can calculate $\text{BF}_{0e}$ by applying the function \texttt{mult\_bf\_equality}. To evaluate $\mathcal{H}_0$, we only need to specify (1) a vector with observed counts, (2) a vector with concentration parameters of the Dirichlet prior distribution, and (3) the vector of proportions expected under the null. Since we have no specific expectations about the distribution of leading digits in the Greek fiscal data, we set all concentration parameters to one which corresponds to a uniform Dirichlet distribution.

```{r, message=FALSE, echo = TRUE, results='hide'}
# Observed counts
x <- c(509, 353, 177, 114,  77,  77,  53,  73,  64)
# Concentration parameters
a <-  rep(1, 9)
# Expected proportions
p <- log10((1:9 + 1)/1:9)
# Execute the analysis
results_H0_He  <- mult_bf_equality(x = x, a = a, p = p)
```

Since the hypotheses $\mathcal{H}_{r1}$ and $\mathcal{H}_{r2}$  contain inequality constraints, we use the function \texttt{mult\_bf\_informed} to compute the Bayes factor of the informed hypotheses to the encompassing hypothesis. In this function, we need to specify (1) a vector with observed counts, (2) the informed hypothesis $\mathcal{H}_{r1}$ or $\mathcal{H}_{r2}$ (e.g., as character vector), (3) a vector with concentration parameters of the Dirichlet prior distribution, and (4) labels for the categories of interest (i.e., leading digits):

```{r, message=FALSE, echo = TRUE, results='hide'}
# Labels for categories of interest
factor_levels <- 1:9
# Specifying the informed Hypothesis
Hr1 <- c('1 > 2 > 3 > 4 > 5 > 6 > 7 > 8 > 9')
Hr2 <- c('1 > 2 = 3 = 4 = 5 = 6 = 7 > 8 > 9')
# Execute the analysis
results_He_Hr1 <- mult_bf_informed(x = x, Hr = Hr1, a = a, 
                                 factor_levels = factor_levels, 
                                 bf_type = 'LogBFr0', seed = 2020)
results_He_Hr2 <- mult_bf_informed(x = x, Hr = Hr2, a = a, 
                                 factor_levels = factor_levels, 
                                 bf_type = 'LogBFr0', seed = 2020)
```

```{r, echo = TRUE}
logbf <- c(results_H0_He$bf$LogBFe0,
           results_He_Hr1$bf_list$bf$LogBFr0,
           results_He_Hr2$bf_list$bf$LogBFr0)
bayes_factor_table <- data.frame(
   BFType = c('LogBFe0', 'LogBFr10', 'LogBFr20'), 
   LogBF  = logbf)
bayes_factor_table
```

As the evidence is extreme in all three cases, we report all Bayes factors on the log scale. The log Bayes factor $\text{log}(\text{BF}_{e0})$ suggests extreme evidence against the hypothesis that the first digits in the Greek fiscal data follow a Benford's distribution; $\text{log}(\text{BF}_{e0}) =$ `r bayes_factor_table[1, 2]`. The log Bayes factor $\text{log}(\text{BF}_{r10})$ indicates extreme evidence in favor for a decreasing trend, $\text{log}(\text{BF}_{r10}) =$ `r bayes_factor_table[2, 2]`. Even though the Bayes factor suggests extreme evidence against the hypothesis that the Greek fiscal data are an empirical dataset, there is no support for the hypothesis that the data are fabricated. The log Bayes factor $\text{log}(\text{BF}_{r20})$ indicates extreme evidence against $\mathcal{H}_{r2}$ with $\text{log}(\text{BF}_{r20}) =$ `r bayes_factor_table[3, 2]`. \Frederik{I must misunderstand something, but this looks to me like extreme evidence \emph{for} $\mathcal{H}_{r2}$?!} When we compare the informed hypotheses directly with each other, the data show evidence for a decreasing trend ($\text{log}(\text{BF}_{r1r2}) =$ `r signif(bayes_factor_table[2,2] - bayes_factor_table[3,2], 3)`).

(ref:benford-caption) The bargraph displays the expected proportions of leading digits according to Benford's law. The black dots indicate for the actual fiscal statistics from Greece the posterior estimates for the proportion of leading digits and the corresponding 95\% credible intervals based on the encompassing model. Only three out of nine estimates cover the expected proportions.

```{r benford, echo = FALSE, message = FALSE, fig.cap = "(ref:benford-caption)"}
xaxis_position <- c(0.8, 2, 3.2, 4.4, 5.6, 6.8, 8, 9.2, 10.4)
estimates <- summary(results_He_Hr1)$estimates
b <- barplot(p, las = 1, xlab = " ", ylab = " ", col = "grey", cex.lab = 1.7,
    cex.main = 1.5, axes = FALSE, ylim = c(0, 0.4))
points(xaxis_position, estimates$median, cex = 1.5, lwd = 2, pch = 19)
#lines(xaxis_position, estimates$median, lwd = 2, type = "c")
l_ply(seq_along(estimates$median), function(x) arrows(x0 = xaxis_position[x], y0 = estimates$lower[x], x1 = xaxis_position[x],
    y1 = estimates$upper[x], code = 3, length = 0.1, angle = 90, lwd = 1.5))
axis(1, xaxis_position, 1:9, cex.axis = 1.3)
axis(2, seq(0, 0.4, by = 0.1), cex.axis = 1.3, las = 1)
#lines(xaxis_position, dat$benford_proportions, lwd = 2, type = "c")
mtext("Leading Digit", side = 1, line = 2.5, cex = 1.5, font = 2)
mtext("Proportion", side = 2, line = 3, cex = 1.5, font = 2)
```

(ref:benford-alt-caption) Proportions of leading digits observed in the fiscal statistics from Greece in comparison to the proportions expected according to Benford's law. The black-rimmed dots indicate the the posterior median estimates and corresponding 95\% credible intervals based on the encompassing model. The grey filled dots indicate the proportions predicted by Benford's law. Only three out of nine estimates cover the expected proportions. This plot was created using the `plot`-S3-method for `summary.bmult` objects. \Frederik{This is a suggestion for an alternative version of the Benford-plot created using the packages plot method and consistent in style with the later plot.}

```{r benford-alt, echo = FALSE, message = FALSE, fig.cap = "(ref:benford-alt-caption)"}
first_digits <- 1:9
benford <- log10((first_digits + 1) / first_digits)

plot(
  summary(results_He_Hr1)
  , xlab = "Leading digit"
  # , ylab = "Proportion"
  # , main = ""
  , panel.first = {
    lines(x = first_digits, y = benford, lty = "22", col = grey(0.5))
    points(x = first_digits, y = benford, pch = 16, col = "white", cex = 2)
    points(x = first_digits, y = benford, pch = 16, bg = "white", col = grey(0.7), cex = 0.8)
  }
)

points(x = first_digits, y = benford, pch = 16, bg = "white", col = grey(0.7), cex = 0.8)

legend(
  "right"
  , legend = c("Benford", "Observed")
  , col = c(grey(0.7), "black")
  , pch = c(16, 21)
  , pt.bg = c(NULL, "white")
  , lty = c("22", "solid")
  , lwd = c(1.25, 1)
  , bty = "n"
  , pt.cex = c(0.8, 1.5)
  , title = "Distribution"
  , seg.len = 1.5
)
```


### Discussion

In this example we tested the data quality of Greek fiscal data in the years 1999 to 2009 by conducting three variations of a Bayesian Benford test. More precisely, we evaluated the null hypothesis that Greek fiscal data conform to Benfords law. We tested this hypothesis against three alternatives. The first alternative hypothesis, $\mathcal{H}_e$ relaxed the constraints imposed by the null hypothesis and left all model parameters free to vary. The second alternative hypothesis, $\mathcal{H}_{r1}$ predicted a decreasing trend in the proportion of leading digits. The third alternative hypothesis $\mathcal{H}_{r2}$ predicted a trend that @hill1988random observed when humans tried to generate random numbers. Our result suggest that the leading digits in the fiscal statistics do not follow a Benford distribution; in fact, we collected extreme evidence against Benford's law compared to two out of three of the alternative hypotheses. When comparing the alternative hypotheses directly to each other, the data show most evidence in favor for a decreasing trend. A Benford test of fiscal statements can be a helpful tool to detect poor data quality and suspicious numbers. In follow-up checks of these numbers, it could then be examined for instance, whether financial statements were actually materially misstated, for instance, by rounding up or down numbers, avoiding certain thresholds etc. [@nigrini2019patterns].

## Example 2: Prevalence of Statistical Reporting Errors

In any scientific article that uses null hypothesis significance testing, there is a chance that the reported test statistic and degrees of freedom do not match the reported $p$-value. In most cases this is because researchers copy the relevant test statistics by hand into their articles and there are no automatic checks to detect these mistakes. Therefore, @epskamp2014statcheck developed the R package \texttt{statcheck}, which only requires the PDF of a given scientific article to detect these reporting errors automatically and efficiently. This package allowed @nuijten2016prevalence to estimate the prevalence of statistical reporting errors in the field of psychology. In total, the authors investigated a sample of $30{,}717$ articles (which translates to over a quarter of a million $p$-values) published in eight major psychological journals between 1985 to 2013: *Developmental Psychology* (DP), the *Frontiers in Psychology* (FP), the *Journal of Applied Psychology* (JAP), the *Journal of Consulting and Clinical Psychology* (JCCP), *Journal of Experimental Psychology: General* (JEPG), the *Journal of Personality and Social Psychology* (JPSP), the *Public Library of Science* (PLoS), *Psychological Science* (PS).

Besides the overall prevalence of statistical reporting errors across these journals, the authors were interested whether there is a higher prevalence for reporting inconsistencies in certain subfields in psychology compared to others. In this context, the possibility was raised that there exists a relationship between the prevalence for reporting inconsistencies and questionable research practices. Specifically, the authors argued that besides honest mistakes when transferring the test statistics into the manuscript, statistical reporting errors occur when authors misreport $p$-values, for instance, by incorrectly rounding them down to or below $0.05$. Based on this assumption, @nuijten2016prevalence predicted that the proportion of statistical reporting errors should be highest in articles published in the *Journal of Personality and Social Psychology* (JPSP), compared to other journals, because compared to other areas of psychology researchers in social psychology most frequently deemed questionable research practices defensible and applicable to their research [@john2012measuring].

### Data and Hypothesis

Here, we reuse the original data published by @nuijten2016prevalence, which we also distribute with the package \textbf{multibridge} under the name `journals`.

```{r, echo = TRUE}
data(journals)
```

The hypothesis of interest, $\mathcal{H}_r$, formulated by @nuijten2016prevalence states that the prevalence for statistical reporting errors for articles published in social psychology journals (i.e., JPSP) is higher than for articles published in other journals. Note that @nuijten2016prevalence did not make use of inferential statistics since their sample included the entire population of articles from the eight flagship journals in psychology from 1985 to 2013. For demonstration purposes, however, we will test the informed hypothesis stated by the authors. We will test $\mathcal{H}_r$ against the the null hypothesis $\mathcal{H}_0$ that all journals have the same prevalence for statistical reporting errors. In this example, the parameter vector of the binomial success probabilities, $\boldsymbol{\theta}$, reflects the probabilities of a statistical reporting error in one of the 8 journals. Thus, we can formalize the discussed hypotheses as follows:

\begin{align*}
    \mathcal{H}_r &: (\theta_{\text{DP}}, \theta_{\text{FP}}, \theta_{\text{JAP}} , \theta_{\text{JCCP}} , \theta_{\text{JEPG}} , \theta_{\text{PLoS}}, \theta_{\text{PS}}) < \theta_{\text{JPSP}} \\
    \mathcal{H}_0 &: \theta_{\text{DP}} =  \theta_{\text{FP}} =  \cdots = \theta_{\text{JPSP}}.
\end{align*}

### Method
To compute the Bayes factor $\text{BF}_{0r}$ we need to specify (1) a vector with observed successes (i.e., number of articles that contain a statistical reporting error), and (2) a vector containing the total number of observations, (3) the informed hypothesis, (4) a vector with prior parameter $\alpha_i$ for each binomial proportion, (5) a vector with prior parameter $\beta_i$ for each binomial proportion, and (6) the category labels (i.e., journal names). Since we have no specific expectations about the distribution of statistical reporting errors across journals, we set all parameters $\alpha_i$ and $\beta_i$ to one which corresponds to uniform beta distributions. With this information, we can now conduct the analysis with the function `binom_bf_informed`. 

```{r, echo=TRUE, message=FALSE, results='hide'}
# Since percentages are rounded to two decimal values, we round the
# articles with an error to obtain integer values
x <- round(journals$articles_with_NHST  * 
             (journals$perc_articles_with_errors/100))

# Total number of articles
n <- journals$articles_with_NHST

# Prior specification
# We assign a uniform beta distribution to each binomial proportion
a <- rep(1, 8)
b <- rep(1, 8)

# Specifying the informed Hypothesis
Hr <- c('JAP , PS , JCCP , PLOS , DP , FP , JEPG < JPSP')

# Category labels
journal_names <- journals$journal

# Execute the analysis
results_H0_Hr <- binom_bf_informed(x = x, n = n, Hr = Hr, a = a, b = b,
                               factor_levels = journal_names,
                               bf_type = 'BF0r', seed = 2020)
```

```{r, echo = TRUE}
bf <- c(results_H0_Hr$bf_list$bf0_table[['BFe0']],
        results_H0_Hr$bf_list$bf[['BFr0']],
        results_H0_Hr$bf_list$bfr_table[['BFre']])
bayes_factor_table <- data.frame(
   BFType = c('BFe0', 'BFr0', 'BFre'), 
   BF = bf)
bayes_factor_table
```

```{r, echo = FALSE}
BFre <- results_H0_Hr$bf_list$bfr_table['BFre']
BFe0 <- results_H0_Hr$bf_list$bf0_table['BFe0']
BFr0 <- results_H0_Hr$bf_list$bf['BFr0']
```


The Bayes factor $\text{BF}_{r0}$ suggests extreme evidence for the informed hypothesis  that the social psychology journal JPSP has the highest prevalence for statistical reporting errors compared to the null hypothesis that the statistical reporting errors are equal across journals; $\log(\text{BF}_{r0}) = `r papaja::printnum(log(BFr0))`$. \Frederik{I, again, must misunderstand something, but this looks to me like extreme evidence \emph{for} $\mathcal{H}_{0}$?!}
When taking a closer look at the Bayes factors, we also see that the data suggest that the null hypothesis that the statistical reporting errors are equal across journals is highly unlikely compared to the encompassing hypothesis, $\log(\text{BF}_{e0}) = `r papaja::printnum(log(BFe0))`$. In addition, the results suggest that the data are $`r papaja::printnum(BFre)`$ more likely under the informed hypothesis than under the hypothesis that the ordering of the journals can vary freely.

In order to get a clearer picture about the ordering of the journals, we can investigate the posterior estimates
under the encompassing model as the next step. The posterior median and 95\% credible interval are returned by the \texttt{summary}-method and can be plotted, Figure\ \@ref(fig:journals).

(ref:journals-caption) The figure displays for each journal the posterior estimates for the prevalence that an article includes a statistical reporting error and the corresponding 95\% credible intervals based on the encompassing model. It appears that all journals show a relatively similar prevalence for statistical reporting errors, with the exception of the *Journal of Applied Psychology* (JAP) and *Psychological Science* (PS), whose prevalence is much lower. This plot was created using the `plot`-S3-method for `summary.bmult` objects.

```{r journals, echo = FALSE, message = FALSE, fig.cap = "(ref:journals-caption)"}
plot(summary(results_H0_Hr), xlab = "Journal")
```

### Discussion

In this example, we tested whether the prevalence of statistical reporting errors for articles published in a social psychology journal (i.e., JPSP) is higher than for articles published in other journals. We tested this hypothesis against the null hypothesis that the prevalence for statistical reporting errors is equal across all journals. The resulting Bayes factor of $\text{BF}_{r0} = `r papaja:::typeset_scientific(signif(BFr0, 3))`$ provides extreme evidence for the informed hypothesis. However, this result should be interpreted with caution. It seems that the result is above all an indication that the null hypothesis is highly misspecified and that the prevalence for a statistical reporting error varies greatly from journal to journal. Evidence that JPSP stands out and has a higher prevalence than the other journals is relatively small; the data provided only moderate evidence against the encompassing hypotheses.

# Summary

The \texttt{R} package \textbf{multibridge} facilitates the estimation of Bayes factors for informed hypotheses in binomial and multinomial models. Compared to existing packages, the packages' efficiently estimates Bayes factors for larger models which occur frequently in empirical studies. This efficient and reliable estimation is made possible by a recently developed bridge sampling routine. The package offers researchers and practitioners the opportunity to specify informed hypotheses that relate closely to their theories. Specifically, informed hypotheses that feature equality constraints, inequality constraints, and free parameters as well as mixtures between them are supported. Moreover, users can also choose whether the informative hypothesis should be tested against an encompassing hypothesis that lets all parameters vary freely or the null hypothesis that states that category proportions are exactly equal.

Beyond the core functions currently implemented in \textbf{multibridge}, there are several natural extensions we aim to include in future versions of this package. For instance, one extension is to facilitate the specification of hierarchical binomial and multinomial models which would allow users to analyze data where responses are nested within participants. Hierarchical multinomial models can be found, for instance, in source memory research where participants need to select a previously studied item from a list of multiple stimuli [e.g., @arnold2019testing]. In addition, we aim to enable the specification of informed hypotheses that are more complex, including hypotheses on the size ratios of the parameters of interest or the difference between category proportions such that informed hypotheses can also be specified on odds ratios. 

\clearpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
