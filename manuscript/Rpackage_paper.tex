% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  english,
  man,floatsintext]{apa6}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={multibridge: An R Package To Evaluate Multinomial Order Constraints},
  pdfauthor={Alexandra Sarafoglou1, Julia M. Haaf1, Frederik Aust1, Eric-Jan Wagenmakers1, \& Maarten Marsman1},
  pdflang={en-EN},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
% Manuscript styling
\usepackage{upgreek}
\captionsetup{font=singlespacing,justification=justified}

% Table formatting
\usepackage{longtable}
\usepackage{lscape}
% \usepackage[counterclockwise]{rotating}   % Landscape page setup for large tables
\usepackage{multirow}		% Table styling
\usepackage{tabularx}		% Control Column width
\usepackage[flushleft]{threeparttable}	% Allows for three part tables with a specified notes section
\usepackage{threeparttablex}            % Lets threeparttable work with longtable

% Create new environments so endfloat can handle them
% \newenvironment{ltable}
%   {\begin{landscape}\begin{center}\begin{threeparttable}}
%   {\end{threeparttable}\end{center}\end{landscape}}
\newenvironment{lltable}{\begin{landscape}\begin{center}\begin{ThreePartTable}}{\end{ThreePartTable}\end{center}\end{landscape}}

% Enables adjusting longtable caption width to table width
% Solution found at http://golatex.de/longtable-mit-caption-so-breit-wie-die-tabelle-t15767.html
\makeatletter
\newcommand\LastLTentrywidth{1em}
\newlength\longtablewidth
\setlength{\longtablewidth}{1in}
\newcommand{\getlongtablewidth}{\begingroup \ifcsname LT@\roman{LT@tables}\endcsname \global\longtablewidth=0pt \renewcommand{\LT@entry}[2]{\global\advance\longtablewidth by ##2\relax\gdef\LastLTentrywidth{##2}}\@nameuse{LT@\roman{LT@tables}} \fi \endgroup}

% \setlength{\parindent}{0.5in}
% \setlength{\parskip}{0pt plus 0pt minus 0pt}

% Overwrite redefinition of paragraph and subparagraph by the default LaTeX template
% See https://github.com/crsh/papaja/issues/292
\makeatletter
\renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries\itshape\typesectitle}}

\renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-\z@\relax}%
  {\normalfont\normalsize\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
\makeatother

% \usepackage{etoolbox}
\makeatletter
\patchcmd{\HyOrg@maketitle}
  {\section{\normalfont\normalsize\abstractname}}
  {\section*{\normalfont\normalsize\abstractname}}
  {}{\typeout{Failed to patch abstract.}}
\patchcmd{\HyOrg@maketitle}
  {\section{\protect\normalfont{\@title}}}
  {\section*{\protect\normalfont{\@title}}}
  {}{\typeout{Failed to patch title.}}
\makeatother
\shorttitle{multibridge}
\DeclareDelayedFloatFlavor{ThreePartTable}{table}
\DeclareDelayedFloatFlavor{lltable}{table}
\DeclareDelayedFloatFlavor*{longtable}{table}
\makeatletter
\renewcommand{\efloat@iwrite}[1]{\immediate\expandafter\protected@write\csname efloat@post#1\endcsname{}}
\makeatother
\usepackage{lineno}

\linenumbers
\usepackage{csquotes}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{nicefrac}
\usepackage{caption}
\usepackage{xcolor}
\definecolor{mypink}{RGB}{255, 230, 255}
\definecolor{myWheat}{RGB}{245, 222, 179}
\definecolor{myGreen}{RGB}{27, 158, 119}
\usepackage{todonotes}
\newcommand{\Julia}[1]{\todo[inline, color=mypink]{#1}}
\newcommand{\Frederik}[1]{\todo[inline, color=myWheat]{#1}}
\newcommand{\Alex}[1]{\todo[inline, color=myGreen]{#1}}
\ifxetex
  % Load polyglossia as late as possible: uses bidi with RTL langages (e.g. Hebrew, Arabic)
  \usepackage{polyglossia}
  \setmainlanguage[]{english}
\else
  \usepackage[shorthands=off,main=english]{babel}
\fi
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newenvironment{cslreferences}%
  {\setlength{\parindent}{0pt}%
  \everypar{\setlength{\hangindent}{\cslhangindent}}\ignorespaces}%
  {\par}

\title{multibridge: An R Package To Evaluate Multinomial Order Constraints}
\author{Alexandra Sarafoglou\textsuperscript{1}, Julia M. Haaf\textsuperscript{1}, Frederik Aust\textsuperscript{1}, Eric-Jan Wagenmakers\textsuperscript{1}, \& Maarten Marsman\textsuperscript{1}}
\date{}


\authornote{

Correspondence concerning this article should be addressed to Alexandra Sarafoglou, Department of Psychology, PO Box 15906, 1001 NK Amsterdam, The Netherlands. E-mail: \href{mailto:alexandra.sarafoglou@gmail.com}{\nolinkurl{alexandra.sarafoglou@gmail.com}}

}

\affiliation{\vspace{0.5cm}\textsuperscript{1} University of Amsterdam}

\abstract{
The \textbf{multibridge} package has been developed to efficiently compute Bayes factors for binomial and multinomial models, that feature inequality constraints, equality constraints, free parameters and mixtures between them. By using the bridge sampling algorithm to compute the Bayes factor, \textbf{multibridge} facilitates the evaluation of large models with many constraints and models with small parameter spaces. The package was developed in the R programming language and is freely available from the Comprehensive \texttt{R} Archive Network (CRAN). We illustrate the functions based on two empirical examples.
}



\begin{document}
\maketitle

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

We present \textbf{multibridge}, an \texttt{R} package to evaluate informed hypotheses in multinomial models and models featuring independent binomials using Bayesian inference. This package allows users to specify constraints on the underlying category proportions including inequality constraints, equality constraints, free parameters and mixtures between them. This package is available from the Comprehensive \texttt{R} Archive Network (CRAN) at \url{https://CRAN.R-project.org/package=multibridge}. Here we introduce the methodology used to evaluate informed hypotheses on categorical variables and show how to use the implementations provided in \textbf{multibridge} through fully reproducible examples.

The most common way to analyze categorical variables is to test whether the underlying category proportions are exactly equal or whether they are fixed and follow a predicted pattern (what is generally known as either chi-square goodness of fit tests, or binomial or multinomial tests). These null hypotheses are then tested against an encompassing hypothesis which places no constraints on the category proportions. This analytic strategy has been criticized, since the null hypotheses might reflect an unrealistic expectation about the real world and the encompassing hypothesis is too uninformative (Hoijtink, Klugkist, \& Boelen, 2008). In addition, this strategy cannot or only insufficiently test the specific predictions derived by scientific theories researchers are interested in. A simple example for this are theories that predict ordinal relations among the underlying category proportions, such as increasing or decreasing trends. For instance, to check for irregularities in audit data, one could test whether the leading digits in the data are distributed according to an expected Benford distribution or whether they deviate from it, for example, by showing a general decreasing trend. Here, the Benford distribution can be tested with standard methods, however, the general decreasing trend cannot be tested, since we cannot derive fixed underlying proportions for the leading digits. Theories can also generate more complex predictions, including ones that feature combinations of equality and inequality constraints, as well as predictions that let some category proportions free to vary. In the following, we will denote such predictions as informed hypotheses, since they ``add theoretical expectations to the traditional alternative hypothesis, thus making it more informative'\,' (Hoijtink et al., 2008, p. 2). Such an informed hypothesis was expressed, for instance, by Nuijten, Hartgerink, Assen, Epskamp, and Wicherts (2016) who studied the prevalence of statistical reporting errors in articles published in different areas of psychological science. Nuijten et al. (2016) hypothesized that articles published in social psychology journals would have higher error rates than articles published in other psychological journals while not expressing expectations about the error rate distribution among the other journals. Here again no fixed proportions can be derived, such that this hypothesis cannot not be evaluated using standard tests. Generally, if researchers can utilize statistical methods for testing informed hypotheses, they are able to specify hypotheses that relate more closely to their theories.

In the Bayesian framework, researchers can compare models that instantiate the hypotheses of interest by means of Bayes factors (Jeffreys, 1935; Kass \& Raftery, 1995). To compute Bayes factors for informed hypotheses several \texttt{R} packages are already available. For instance, with the package \textbf{multinomineq} (Heck \& Davis-Stober, 2019) users can specify inequality constrained hypotheses but also more general linear inequality constraints for multinomial models as well as models that feature independent binomials. The \textbf{BAIN} package (Gu, Hoijtink, Mulder, \& Rosseel, 2019) allows for the evaluation of inequality constraints in structural equation models. Outside of \texttt{R}, the Fortran 90 program \textbf{BIEMS} (Mulder, Hoijtink, Leeuw, \& others, 2012) allows for the evaluation of order constraints for multivariate linear models such as MANOVA, repeated measures, and multivariate regression. All these packages rely on one of two methods to approximate order constrained Bayes factors: the encompassing prior approach (Gu, Mulder, Deković, \& Hoijtink, 2014; Hoijtink, 2011; Hoijtink et al., 2008; Klugkist, Kato, \& Hoijtink, 2005) and the conditioning method (Mulder, 2014, 2016; Mulder et al., 2009). However, even though these methods are currently widely used, they are known to become increasingly unreliable and inefficient as the number of constraints increases or when the parameter space of the constrained model is small (Sarafoglou et al., 2020).

In contrast to these available packages, \textbf{multibridge} uses a bridge sampling routine that enables users to compute Bayes factors for informed hypotheses more reliably and efficiently (Bennett, 1976; Meng \& Wong, 1996; Sarafoglou et al., 2020). The workhorse for this analysis, the bridge sampling algorithm, constitutes a special case of the algorithm implemented in the \texttt{R} package \textbf{bridgesampling} (Gronau, Singmann, \& Wagenmakers, 2020). The \textbf{bridgesampling} package, allows users to estimate the marginal likelihood for a wide variety of models, including models implemented in Stan (Stan Development Team, 2020). However, the algorithm implemented in \textbf{bridgesampling} is not suitable for models that include constraints on probability vectors and hence is unsuitable for the analysis of categorical data. Therefore, in \textbf{multibridge}, we tailored the bridge sampling algorithm such that it accommodates the specification of informed hypotheses on probability vectors. The package then produces an estimate for the Bayes factor in favor of or against the informed hypothesis. The resulting Bayes factor compares the evidence for the informed hypotheses to the encompassing hypothesis that imposes no constraints on the underlying category proportions. Alternatively, the informed hypothesis can be tested against the null hypothesis that all underlying category proportions are exactly equal. Given this result, users can then either receive a visualization of the posterior parameter estimates under the encompassing hypothesis using the \texttt{plot}-method, or get more detailed information on how the Bayes factor is composed using the \texttt{summary}-method. For hypotheses that include mixtures between equality and inequality constrained hypotheses the \texttt{bayes\_factor} method shows the conditional Bayes factor for the inequality constraints given the equality constraints and a Bayes factor for the equality constraints. The general workflow of \textbf{multibridge} is illustrated in Figure \ref{fig:scheme-multibridge}. Table \ref{table:s3_methods} summarizes all S3 methods currently available in \textbf{multibridge}.



\begin{figure}
\includegraphics[width=400px]{scheme_multibridge} \caption{The \textbf{multibridge} workflow. The user specifies the data values (\texttt{x} and \texttt{n} for binomial models and \texttt{x} for multinomial models, respectively), the informed hypothesis (\texttt{Hr}), the \(\alpha\) and \(\beta\) parameters of the Binomial prior distributions (\texttt{a} and \texttt{b}) or the concentration parameters for the Dirichlet prior distribution (\texttt{a}), respectively, and the factor levels (\texttt{factor\_levels}). The functions \texttt{mult\_bf\_informed} and \texttt{binom\_bf\_informed} then produce an estimate for the Bayes factor of the informed hypothesis versus the encompassing or the null hypothesis. Based on these results different S3 methods can be used to get more detailed information on the individual components of the analysis (e.g., \texttt{summary}, \texttt{bayes\_factor}), and parameter estimates of the encompassing distribution (\texttt{plot}).}\label{fig:scheme-multibridge}
\end{figure}

\begin{table}
\caption {S3 methods available in $\textbf{multibridge}$}
\label{table:s3_methods}
\begin{center}
\begin{tabular}{p{4cm}p{3.5cm}p{9cm}}
        \toprule
Function Name(s) & S3 Method & Description \\\midrule
$\texttt{mult\_bf\_informed}$, $\texttt{binom\_bf\_informed}$ & $\texttt{print}$ & Prints model specifications and descriptives. \\
 & $\texttt{summary}$ &  Prints and returns the Bayes factor and associated hypotheses for the full model, and all equality and inequality constraints.\\
  & $\texttt{plot}$ &  Plots the posterior median and 95\% credible interval of the parameter estimates of the encompassing model.\\
 & $\texttt{bayes\_factor}$ & Contains all Bayes factors and log marginal likelihood estimates for inequality constraints.\\
 & $\texttt{samples}$ & Extracts prior and posterior samples from constrained distribution (if bridge sampling was applied). \\
& $\texttt{bridge\_output}$    &  Extracts bridge sampling output and associated error measures.\\
& $\texttt{restriction\_list}$ & Extracts restriction list and associated informed hypothesis. \\
$\texttt{binom\_bf\_inequality}$, $\texttt{binom\_bf\_inequality}$  & $\texttt{print}$ & Prints the bridge sampling estimate for the log marginal likelihood and the corresponding percentage error. \\
& $\texttt{summary}$ & Prints and returns the bridge sampling estimate for the log marginal likelihood and associated error terms.\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

The remainder of this article is organized as follows: In the methods section, we describe the Bayes factor identity for informed hypotheses in binomial and multinomial models, and present the bridge sampling routine implemented in the \textbf{multibridge} package including details of the necessary transformations required for this routine. In Section 3, we will schematically introduce the most relevant functions in \textbf{multibridge} and their arguments. Section 4 illustrates how to use the \textbf{multibridge} package to estimate parameters, and compute Bayes factors using two examples.

\hypertarget{methods}{%
\section{Methods}\label{methods}}

In this section we formalize multinomial models and models that feature independent binomial probabilities as we have implemented them in \textbf{multibridge}. In the multinomial model, we assume that the vector of observations \textbf{x} in the \(K\) categories follow a multinomial distribution in which the parameters of interest, \(\boldsymbol{\theta}\), represent the underlying category proportions. Since we assume a dependence between the \(K\) categories, the vector of probability parameters is sum-to-one constrained, such that \(\sum_{k = 1}^K (\theta_1, \cdots, \theta_K) = 1\). Therefore, a suitable choice for a prior distribution for \(\boldsymbol{\theta}\) is the Dirichlet distribution with concentration parameters \(\boldsymbol{\alpha}\):

\begin{align}
  x_1, \cdots, x_K &\sim \text{Multinomial}(\sum_{k = 1}^K x_k, \theta_1, \cdots, \theta_K) \\
  \theta_1, \cdots, \theta_K &\sim \text{Dirichlet}(\alpha_1, \cdots, \alpha_K),
\end{align}

where \(\boldsymbol{\alpha}\) can be interpreted as vector of \emph{a priori} category counts. Since the multinomial model constitutes a generalization of the binomial model (for \(K \geq 2\)), the formalization of a model that features independent binomial probabilities is very similar. In the binomial model, we assume that the elements in the vector of successes \textbf{x} and the elements in the vector of total number of observations \textbf{n} in the \(K\) categories follow independent binomial distributions. As in the multinomial model, the parameter vector of the binomial success probabilities \(\boldsymbol{\theta}\) contains the underlying category proportions, however, in this model we assume that categories are independent which removes the sum-to-one constraint. Therefore, a suitable choice for a prior distribution for \(\boldsymbol{\theta}\) is a vector of independent beta distributions with parameters \(\boldsymbol{\alpha}\) and \(\boldsymbol{\beta}\):

\begin{align}
  x_1 \cdots x_K & \sim \prod_{k = 1}^K \text{Binomial}(\theta_k, n_k) \\
  \theta_1 \cdots \theta_K &\sim \prod_{k = 1}^K \text{Beta}(\alpha_k, \beta_k),
\end{align}
where \(\boldsymbol{\alpha}\) can be interpreted as vector of \emph{a priori} successes that observations fall withing the various categories and \(\boldsymbol{\beta}\) can be interpreted as vector of \emph{a priori} failures.

\hypertarget{bayes-factor}{%
\subsection{Bayes factor}\label{bayes-factor}}

With \textbf{multibridge} package, it is possible to collect evidence for informed hypotheses on a parameter vector \(\boldsymbol{\theta}\) by means of the Bayes factor. Bayes factors compare the relative evidence of two hypotheses in the light of the data. It is defined as the ratio of marginal likelihoods of the respective hypotheses. For instance, the Bayes factor for the informed hypothesis versus a hypothesis that lets all parameters free to vary is defined as:

\begin{align*}
\text{BF}_{re} = \cfrac{\overbrace{p(\mathbf{x}\mid \mathcal{H}_r)}^{\substack{\text{Marginal likelihood}\\\text{under $\mathcal{H}_r$}}}}{\underbrace{p(\mathbf{x}\mid \mathcal{H}_e)}_{\substack{\text{Marginal likelihood}\\\text{under $\mathcal{H}_e$}}}},
\end{align*}

where the subscript \(r\) denotes the informed hypothesis and \(e\) denotes a hypothesis which predicts that all parameters free to vary. In \textbf{multibridge} we use two different methods to compute Bayes factors, one method evaluates hypotheses that feature equality constraints on \(\boldsymbol{\theta}\) and one method evaluates hypotheses that feature inequality constraints on \(\boldsymbol{\theta}\). Both methods will be outlined below. In cases where informed hypotheses feature mixtures between inequality and equality constraints, we compute the corresponding Bayes factor \(\text{BF}_{re}\), by multiplying the individual Bayes factors for both constrait types with each other:

\[
\text{BF}_{re}
= \text{BF}_{1e} \times \text{BF}_{2e} \mid \text{BF}_{1e},
\]
where the subscript \(1\) denotes the hypothesis that only features equality constraints and the subscript \(2\) denotes the hypothesis that only features inequality constraints. A Bayes factor for mixtures thus factors into a Bayes factor for the equality constraints, \(\text{BF}_{1e}\), and a conditional Bayes factor for the inequality constraints given the equality constraints \(\text{BF}_{2e} \mid \text{BF}_{1e}\) (for the proof, see Sarafoglou et al., 2020).

\hypertarget{the-bayes-factor-for-equality-constraints}{%
\subsection{The Bayes Factor For Equality Constraints}\label{the-bayes-factor-for-equality-constraints}}

The Bayes factor for the equality constraints can be computed analytically both for binomial and multinomial models. For binomial models, the function \texttt{binom\_bf\_equality} is available to compute \(\text{BF}_{0e}\). Assuming that the first \(i\) binomial probabilities in a model are equality constrained, the Bayes factor is defined as:
\begin{align*}
\text{BF}_{0e} &=
&= \cfrac{\prod_{i < k} \text{B}(\alpha_i\text{, } \beta_i)}{\prod_{i < k} \text{B}(\alpha_i + x_i\text{, } \beta_i + n_i - x_i)} \times \cfrac{\text{B}(\alpha_+ + x_+ - i + 1\text{, } \beta_+ + n_+ - x_+ - i + 1)}{\text{B}(\alpha_+ - i + 1\text{, } - i + 1)}
\end{align*}
where \(\text{B}()\) denotes the beta function and \(\alpha_+ = \sum_{i<k}\alpha_i\), \(\beta_+ = \sum_{i<k}\beta_i\), \(x_+ = \sum_{i<k} x_i\) and \(n_+ = \sum_{i<k} n_i\). The latter factor introduces a correction for marginalizing which stems from the change in degrees of freedom, when we collapse \(i\) equality constraint parameters: For \(i\) collapsed categories, \(i - 1\) degrees of freedom are lost which are subtracted from the prior parameters in the corresponding Binomial distribution.

For multinomial models, the function \texttt{multBayes\_bf\_equality} is available. Assuming again that the first \(i\) category probabilities in a model are equality constraint, the Bayes factor \(\text{BF}_{=e}\) is defined as:
\begin{align*}
\text{BF}_{0e} = \frac{\text{B}(\boldsymbol{\alpha}+\mathbf{x})}{\text{B}(\boldsymbol{\alpha})} \, \left(\frac{1}{i}\right)^{\sum_{i<k} x_i}\,\frac{
 \text{B}\left(\sum_{i<k}\alpha_i - i + 1\text{, }\alpha_{k}\text{, }\dots\text{, }\alpha_K\right)}{\text{B}\left(\sum_{i<k}\alpha_i+x_i - i + 1\text{, }\alpha_{k}+x_{k}\text{, }\dots\text{, }\alpha_K+x_K\right)},
\end{align*}

\hypertarget{the-bayes-factor-for-inequality-constraints}{%
\subsection{The Bayes Factor For Inequality Constraints}\label{the-bayes-factor-for-inequality-constraints}}

To approximate the Bayes factor for informed hypotheses, Klugkist et al. (2005) derived the following identity:

\begin{align}
\label{Eq:klugkistIdentity}
\text{BF}_{re} &= \cfrac{\overbrace{p(\boldsymbol{\theta} \in \mathcal{R}_r \mid \mathbf{x}\text{, }\mathcal{H}_e)}^{\substack{\text{Proportion of posterior parameter}\\\text{space consistent with the restriction}}}}{\underbrace{p(\boldsymbol{\theta} \in \mathcal{R}_r \mid  \mathcal{H}_e)}_{\substack{\text{Proportion of prior parameter}\\\text{space consistent with the restriction}}}}.
\end{align}

Recently, Sarafoglou et al. (2020) showed that the Bayes factor \(\text{BF}_{re}\) can also be interpreted as ratio of two marginal likelihoods:

\begin{align}
\label{Eq:sarafoglouIdentity}
\text{BF}_{re} =
\cfrac{\overbrace{p(\boldsymbol{\theta} \in \mathcal{R}_r \mid \mathbf{x}\text{, }\mathcal{H}_e)}^{\substack{\text{Marginal likelihood of}\\\text{posterior distribution}}}}{\underbrace{p(\boldsymbol{\theta} \in \mathcal{R}_r \mid  \mathcal{H}_e)}_{\substack{\text{Marginal likelihood of}\\\text{prior distribution}}}}.
\end{align}

In this identity, \(p(\boldsymbol{\theta} \in \mathcal{R}_r \mid \mathbf{x}\text{, }\mathcal{H}_e)\) denotes the marginal likelihood of the constrained posterior distribution and \(p(\boldsymbol{\theta} \in \mathcal{R}_r \mid \mathcal{H}_e)\) denotes the marginal likelihood of the constrained prior distribution. Even though both identities are mathematically equivalent, the methods to estimate these identities are very different.
In the first case, for instance, the number of samples from the encompassing distribution in accordance with the inequality constrained hypothesis, serve as an estimate for the proportion of prior parameter space consistent with the restriction. On the flip side, however, this means that the accuracy of this estimate is strongly dependent on the number of the constrained parameters in the model and the size of the constrained parameter space. That is, as the constraints become stronger, the constrained parameter space decreases. As a result it becomes less likely that draws from the encompassing distribution will fall into the constrained region, so that in some cases the estimation of the Bayes factor becomes practically impossible (Sarafoglou et al., 2020).

However, when we interpret the Bayes factor \(\text{BF}_{re}\) as ratio of marginal likelihoods and we are able to sample from the constrained prior and posterior distributions, we can utilize numerical sampling methods such as bridge sampling to obtain the estimates. Crucially, in this approach, it does not matter how small the constrained parameter space is in proportion to the encompassing density. This gives the method a decisive advantage over the encompassing prior approach in terms of accuracy and efficiency especially (1) when binomial and multinomial models with relatively high number of categories (i.e., \(K > 10\)) are evaluated and (2) when relatively little posterior mass falls in the constrained parameter space.

\hypertarget{the-bridge-sampling-method}{%
\subsection{The Bridge Sampling Method}\label{the-bridge-sampling-method}}

Bridge sampling is a method to estimate the ratio of two marginal likelihoods (Bennett, 1976; Meng \& Wong, 1996). In \textbf{multibridge}, we are using bridge sampling to estimate the identity presented in Equation \ref{Eq:sarafoglouIdentity}. But instead of estimating the ratio of marginal likelihoods directly, we implemented a version of bridge sampling that estimates one marginal likelihood at the time. This approach has the benefit that it increases the accuracy of the method without considerably increasing its computational efficiency (Overstall \& Forster, 2010). Specifically, we subsequently estimate the marginal likelihood for the constrained prior distribution and the marginal likelihood of the constrained posterior distribution.

When applying this modified version of the bridge sampling method, we estimate each marginal likelihood by means of a so-called proposal distribution. In \textbf{multibridge} this proposal distribution is the multivariate normal distribution. To estimate the marginal likelihood, bridge sampling only requires samples from the distribution of interest---the so-called target distribution---and samples from the proposal distribution.

Samples from the target distribution---that is the constrained prior and posterior Dirichlet distribution for multinomial models and constrained prior and posterior beta distributions for binomial models---are drawn through the Gibbs sampling algorithms proposed by Damien and Walker (2001). For binomial models, we apply the suggested Gibbs sampling algorithm for constrained beta distributions. In the case of the multinomial models, we apply an algorithm that simulates values from constrained Gamma distributions which are then transformed into Dirichlet random variables (for details, see Appendix C in Sarafoglou et al., 2020). To sample efficiently from these distributions, we implemented a \texttt{C++} routine for this algorithm in the package.

Samples from the proposal distribution can be generated using the standard \texttt{rmvnorm}-function from the \texttt{R} package \textbf{stats}. The vector of means and the covariance matrix of this distribution are derived from one part of the samples of the probit transformed target distribution. The reason for this approach is that the efficiency of the bridge sampling method is optimal only if the target and proposal distribution operate on the same parameter space and have sufficient overlap. We therefore probit transform the samples of the constrained distributions to move the samples from the probability space to the entire real line. Subsequently, we use half of these draws to construct the proposal distribution using the method of moments. Details on the probit transformations are provided in the appendix. Thus, for the marginal likelihood of the constrained prior distribution, the modified bridge sampling identity is then defined as:

\begin{align}
    p(\boldsymbol{\theta} \in \mathcal{R}_r \mid \mathcal{H}_e) = \cfrac{\mathbb{E}_{g(\boldsymbol{\theta})}\left(p(\boldsymbol{\theta}\mid \mathcal{H}_e) \mathbb{I}(\boldsymbol{\theta}\in\mathcal{R}_r)h(\boldsymbol{\theta})\right)}{\mathbb{E}_{\text{prior}} \left(g(\boldsymbol{\theta})h(\boldsymbol{\theta})\right)},
    \label{Eq:bridgeidentity}
\end{align}
where the term \(h(\boldsymbol{\theta})\) refers to the bridge function proposed by Meng and Wong (1996) and \(g(\boldsymbol{\theta})\) refers to the proposal distribution. The numerator evaluates the unnormalized density for the constrained prior distribution with samples from the proposal distribution. The denominator evaluates the normalized proposal distribution with samples from the constrained prior distribution. Using this identity, we receive the bridge sampling estimator for the marginal likelihood of the constrained prior distribution by applying the iterative scheme proposed by Meng and Wong (1996):

\begin{align*}
    \hat p(\boldsymbol{\theta} \in \mathcal{R}_r \mid \mathcal{H}_e)^{(t+1)} \approx \cfrac{\cfrac{1}{N_2} \sum_{m = 1}^{N_2} \cfrac{\ell_{2,m}}{s_1 \ell_{2,m} + s_2 p(\boldsymbol{\tilde \theta_m} \in \mathcal{R}_r \mid \mathcal{H}_e)^{(t)}}}
    {\cfrac{1}{N_1} \sum_{n = 1}^{N_1} \cfrac{1}{s_1 \ell_{1,n} + s_2 p(\boldsymbol{\theta^*_n} \in \mathcal{R}_r \mid \mathcal{H}_e)^{(t)}}},
    %\label{Eq:bridgeIterativeScheme}
\end{align*}
where \(N_1\) denotes the number of samples drawn from the constrained distribution, that is, \(\boldsymbol{\theta}^* \sim p(\boldsymbol{\theta} \mid \mathcal{H}_r)\), \(N_2\) denotes the number of samples drawn from the proposal distribution, that is \(\boldsymbol{\tilde \theta} \sim g(\boldsymbol{\theta})\),
\(s_1 = \frac{N_1}{N_2 + N_1}\), and \(s_2 = \frac{N_2}{N_2 + N_1}\). The quantities \(\ell_{1,n}\) and \(\ell_{2,m}\) are defined as follows:

\begin{align}
    \ell_{1,n} &= \cfrac{q_{1,1}}{q_{1,2}}  = \cfrac{p(\boldsymbol{\theta^*_n}\mid \mathcal{H}_e) \mathbb{I}(\boldsymbol{\theta^*_n}\in\mathcal{R}_r)}{g(\boldsymbol{\xi_n}^*)},\\
    \ell_{2,m} &= \cfrac{q_{2,1}}{q_{2,2}} = \cfrac{p(\boldsymbol{\tilde \theta_m}\mid \mathcal{H}_e) \mathbb{I}(\boldsymbol{\tilde \theta_m}\in\mathcal{R}_r)}{g(\boldsymbol{\tilde \xi_m})},
\end{align}
where \(\boldsymbol{\xi_n}^* = \Phi^{-1}\left(\cfrac{\boldsymbol{\theta^*_n} - \mathbf{l}}{\mathbf{u} - \mathbf{l}}\right)\), and \(\boldsymbol{\tilde \theta_m} = ((\mathbf{u} - \mathbf{l})\Phi(\boldsymbol{\tilde \xi_m}) + \mathbf{l}) \left|J\right|)\). The quantity \(q_{1,1}\) refers to the evaluations of the constrained distribution for constrained samples and \(q_{1,2}\) refers to the proposal evaluations for constrained samples, respectively. The quantities \(q_{2,1}\) refers to evaluations of the constrained distribution for samples from the proposal and \(q_{2,2}\) refers to the proposal evaluations for samples from the proposal, respectively. Note that the quantities \(\ell_{1,n}\) and \(\ell_{2,m}\) have been adjusted to account for the necessary parameter transformations to create overlap between the constrained distributions and the proposal distribution. \textbf{multibridge} runs the iterative scheme until the tolerance criterion suggested by Gronau et al. (2017) is reached, that is:
\begin{align*}
\cfrac{\mid \hat p(\boldsymbol{\theta} \in \mathcal{R}_r \mid \mathcal{H}_e)^{(t + 1)} - \hat p(\boldsymbol{\theta} \in \mathcal{R}_r \mid \mathcal{H}_e)^{(t)} \mid}{\hat p(\boldsymbol{\theta} \in \mathcal{R}_r \mid \mathcal{H}_e)^{(t + 1)}} &\leq 10^{-10}.
\end{align*}

The bridge sampling estimate for the log marginal likelihood of the constrained distribution and its associate relative mean square error, the number of iterations, and the quantities \(q_{1,2}\), \(q_{1,2}\), \(q_{1,2}\), and \(q_{1,2}\) are included in the standard output in \textbf{multibridge}. The function to compute the relative mean square error was taken from the R package \textbf{bridgesampling}.\todo[inline, color=myWheat]{Is this important enough to mention it here?}\todo[inline, color=myGreen]{Not sure where to include it otherwise}

\hypertarget{usage-and-examples}{%
\section{Usage and Examples}\label{usage-and-examples}}

The \textbf{multibridge} package can be installed from the Comprehensive R Archive Network (CRAN) at
\url{https://CRAN.R-project.org/package=multibridge}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\StringTok{\textquotesingle{}multibridge\textquotesingle{}}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{\textquotesingle{}multibridge\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

A list of all currently available functions and datasets is given in Table \ref{table:core_functions}. Additional examples are available as vignettes (see \url{https://cran.r-project.org/package=multibridge}, or \texttt{vignette(package\ =\ "multibridge")}). The two core functions of \textbf{multibridge}---the \texttt{mult\_bf\_informed}-function and the \texttt{binom\_bf\_informed}-function---can be illustrated schematically as follows:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mult\_bf\_informed}\NormalTok{(x, Hr, a factor\_levels)}
\KeywordTok{binom\_bf\_informed}\NormalTok{(x, n, Hr, a, b, factor\_levels)}
\end{Highlighting}
\end{Shaded}

The basic required arguments for these functions are listed in Table \ref{tab:arguments}. In the following, we will outline two examples on how to use \textbf{multibridge} to compare an informed hypothesis to a null or encompassing hypothesis. In addition, the first example shows how two informed hypotheses can be compared to each other.

\begin{table}
\caption{To estimate the Bayes factor in favor for or against the specified informed hypothesis, the user provides the core functions \texttt{mult\_bf\_informed} and \texttt{binom\_bf\_informed} with the following basic required arguments}
\label{tab:arguments}
\begin{center}
\begin{tabular}{p{4cm}p{12cm}}
        \toprule
Argument & Description \\\midrule
\texttt{x} & a vector with data (for multinomial models) or a vector of counts of successes, or a two-dimensional table (or matrix) with 2 columns, giving the counts of successes and failures, respectively (for binomial models)  \\
\texttt{n} &  numeric. Vector of counts of trials. Must be the same length as \texttt{x}. Ignored if \texttt{x} is a matrix or a table \\
\texttt{Hr} & string or character. Encodes the user specified informed hypothesis. Users can either use the specified \texttt{factor\_levels} or indexes to refer to parameters.\\
\texttt{a} & numeric. Vector with concentration parameters of Dirichlet distribution (for multinomial models) or $\alpha$ parameters for independent beta distributions (for binomial models). Default sets all parameters to 1 \\
\texttt{b} & numeric. Vector with $\beta$ parameters. Must be the same length as \texttt{x}. Default sets all $\beta$ parameters to 1 \\
\texttt{factor\_levels} &  character. Vector with category names. Must be the same length as \texttt{x}\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\begin{table}
\caption {Core functions available in $\textbf{multibridge}$}
\label{table:core_functions}
\begin{center}
\begin{tabular}{p{5cm}p{11cm}}
        \toprule
Function Name(s) & Description \\\midrule
$\texttt{mult\_bf\_informed}$ & Evaluates informed hypotheses on multinomial parameters.  \\
$\texttt{mult\_bf\_inequality}$ & Estimates the marginal likelihood of a constrained prior or posterior Dirichlet distribution.  \\
$\texttt{mult\_bf\_equality}$ & Computes Bayes factor for equality constrained multinomial parameters using the standard Bayesian multinomial test.  \\
$\texttt{mult\_tsampling}$ & Samples from truncated prior or posterior Dirichlet density.\\
$ \texttt{lifestresses}, \texttt{peas}$ & Datasets associated with informed hypotheses in multinomial models.\\\midrule
$\texttt{binom\_bf\_informed}$ & Evaluates informed hypotheses on binomial parameters.  \\
$\texttt{binom\_bf\_inequality}$ & Estimates the marginal likelihood of constrained prior or posterior beta distributions.\\
$\texttt{binom\_bf\_equality}$ & Computes Bayes factor for equality constrained binomial parameters.  \\
$\texttt{binom\_tsampling}$ & Samples from truncated prior or posterior beta densities.\\
$ \texttt{journals}$ & Dataset associated with informed hypotheses in binomial models.\\\midrule
$ \texttt{generate\_restriction\_list}$ & Encodes the informed hypothesis.\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\hypertarget{example-1-applying-a-benford-test-to-greek-fiscal-data}{%
\subsection{Example 1: Applying A Benford Test to Greek Fiscal Data}\label{example-1-applying-a-benford-test-to-greek-fiscal-data}}

The first digit phenomenon, otherwise known as Benford's law (Benford, 1938; Newcomb, 1881) states that the expected proportion of leading digits in empirical data can be formalized as follows: for any given leading digit \(d, d = (1, \cdots, 9)\) the expected proportion is approximately equal to \[\mathbb{E}_{\theta_d}= \text{log}_{10}((d + 1)/d).\] This means that a number in a empirical dataset has leading digit \(1\) in \(30.1 \%\) of the cases, and leading digit \(2\) in \(17.61 \%\) of the cases; leading digit \(9\) is the least frequent digit with an expected proportion of only \(4.58 \%\) (see Table \ref{Tab:benford} for an overview of the expected proportions). Empirical data includes, for instance, data on population sizes, death rates, baseball statistics, atomic weights of elements, and physical constants (Benford, 1938). In contrast, generated data, such as telephone numbers, do in general not obey Benford's law (Hill, 1995). Since Benford's law proved to be highly suitable to discriminate between empirical data and generated data, a so-called Benford test can be used to check whether certain observed frequencies of first digits, obey Benford's law and therefore can be considered an empirical dataset. Benford's tests are used in fields like accounting and auditing to check for indications for poor data quality, for instance, in fiscal statements {[}for an overview, {]}see e.g., Durtschi, Hillison, and Pacini (2004), Nigrini and Mittermaier (1997), Nigrini (2012){]}. Data that do not pass the Benford test, should raise audit risk concerns, meaning that it is recommended that the data undergo additional follow-up checks (Nigrini, 2019).

In the following, we discuss three possible Bayesian adaptations of the Benford's test. In a first scenario we simply conduct Bayesian multinomial test in which we test the point-null hypothesis \(\mathcal{H}_0\) which predicts a Benford distribution against the encompassing hypothesis \(\mathcal{H}_{e}\) which leaves all model parameters free to vary. Testing against the encompassing hypothesis is considered standard practice, yet, it leads to an unfair comparison to the detriment of the null hypothesis. In general, if we are dealing with a high-dimensional parameter space and the competing hypotheses differ largely in their complexity, the Bayes factor generally favors the less complex hypothesis (i.e., \(\mathcal{H}_{e}\)) even if the data follow the predicted trend of the more complex hypothesis considerably well. In a second scenario we therefore test the null hypothesis against an alternative hypothesis, denoted as \(\mathcal{H}_{r1}\), which predicts a decreasing trend in the proportions of leading digits. The hypothesis \(\mathcal{H}_{r1}\) implies considerably more constraints than \(\mathcal{H}_{e}\) and is a suitable choice if our primary goal is to distinguish whether data comply with Benford's law or whether the data only follow a similar trend. In a third scenario, where the main goal is to identify fabricated data, we could test the null hypothesis against an hypothesis, which predicts a trend that is characteristic for manipulated data. This hypothesis, which we denote as \(\mathcal{H}_{r2}\), could be derived from empirical research on fraud or be based on observed patterns from former fraud cases. For instance, Hill (1988) instructed students to produce a series of random numbers; in the resulting data the proportion of the leading digit \(1\) occurred most often and the digits \(8\) and \(9\) occurred least often which is consistent with the general pattern of Benford's law. However, the proportion for the remaining leading digits were approximately equal. We do want to note that the predicted distribution derived from Hill (1988) is not currently used as a test to detect fraud. However, for the sake of simplicity, if we assume that this pattern could be an indication for fabricated auditing data, the Bayes factor could quantify the evidence of whether the proportion of first digits resemble authentic or fabricated data.

\hypertarget{data-and-hypothesis}{%
\subsubsection{Data and Hypothesis}\label{data-and-hypothesis}}

The data we use to illustrate the computation of Bayes factors were originally published by the European statistics agency ``Eurostat'' and served as basis for reviewing the adherence to the Stability and Growth Pact of EU member states. Rauch, Göttsche, Brähler, and Engel (2011) conducted a Benford test on data related to budget deficit criteria, that is, public deficit, public dept and gross national products. This data used for this example contains fiscal data from Greece related in the years between \(1999\) and \(2010\); a total of \(N= 1{,}497\) numerical data were included in the analysis. We choose this data, since the Greek government deficit and debt statistics states has been repeatedly criticized by the European Commission in this timespan (European Commision, 2004, 2010). In particular, the commission has accused the Greek statistical authorities to have misreported deficit and debt statistics. For further details on the dataset see Rauch et al. (2011). The observed proportions are displayed in Table \ref{Tab:benford}, the figure displaying the observed versus the expected proportions are displayed in Figure \ref{fig:benford}.

\begin{table}[h]
    \centering
    \caption{The Table shows the Observed Counts, Observed Proportions, and Expected Proportions of first digits in Greece governmental data. The total sample size was $N = 1{,}497$ observations. Note that the observed proportions and counts deviate slightly from those reported in Rauch et al. (2011) (probably due to rounding errors).}
    \begin{tabular}{cccp{4cm}}
        \hline
Leading digit & Observed Counts & Observed Proportions & Expected Proportions: Benford's Law  \\
        \hline
        1 & 509 & 0.340 & 0.301  \\
        2 & 353 & 0.236 & 0.176  \\
        3 & 177 & 0.118 & 0.125  \\
        4 & 114 & 0.076 & 0.097  \\
        5 & 77 & 0.051 & 0.079  \\
        6 & 77 & 0.051 & 0.067  \\
        7 & 53 & 0.035 & 0.058  \\
        8 & 73 & 0.049 & 0.051  \\
        9 & 64 & 0.043 & 0.046  \\
        \hline
    \end{tabular}
    \label{Tab:benford}
\end{table}

In this example, the parameter vector of the multinomial model, \(\theta_1, \cdots, \theta_K\), reflects the probabilities of a leading digit in the Greek fiscal data being a number from \(1\) to \(9\). Thus, we can formalize the discussed hypotheses as follows. The null hypothesis specifies that the proportions of first digits obeys Benford's law:
\[\mathcal{H}_0 : \boldsymbol{\theta}_0 = (0.301, 0.176, 0.125, 0.097, 0.079, 0.067, 0.058, 0.051, 0.046).\]

We are testing the null hypothesis against the following alternative hypotheses:
\begin{align*}
\mathcal{H}_e &: \boldsymbol{\theta} \sim \text{Dirichlet}(\boldsymbol{\alpha}), \\
\mathcal{H}_{r1} &: \theta_1 > \theta_2 > \theta_3 > \theta_4 > \theta_5 > \theta_6 > \theta_7 > \theta_8 > \theta_9, \\
\mathcal{H}_{r2} &:  \theta_1 > (\theta_2 = \theta_3 = \theta_4 = \theta_5 = \theta_6 = \theta_7) > (\theta_8, \ \theta_9).
\end{align*}

In cases, in which we are interested in computing two informed hypotheses with each other, we need to make use of the transitivity property of the Bayes factor. For instance, if we would like to compare the two informed hypotheses \(\mathcal{H}_{r1}\) and \(\mathcal{H}_{r2}\) with each other, we would first compute \(\text{BF}_{er1}\) and \(\text{BF}_{er2}\) and then yield \(\text{BF}_{r1r2}\) as follows:
\[\text{BF}_{r1e} \times \text{BF}_{er2} = \text{BF}_{r1r2}.\]

\hypertarget{method}{%
\subsubsection{Method}\label{method}}

We can compare \(\mathcal{H}_0\) and \(\mathcal{H}_e\) by means of a Bayesian multinomial test, that is, we stipulate equality constraints on the entire parameter vector \(\boldsymbol{\theta}\). The corresponding Bayes factor is thus computationally straightforward; we can calculate \(\text{BF}_{0e}\) by applying the function \texttt{mult\_bf\_equality}. To evaluate \(\mathcal{H}_0\), we only need to specify (1) a vector with observed counts, (2) a vector with concentration parameters of the Dirichlet prior distribution, and (3) the vector of predicted proportions. Since we have no specific expectations about the distribution of leading digits in the Greek fiscal data, we set all concentration parameters to one which corresponds to a uniform Dirichlet distribution.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Observed counts}
\NormalTok{x \textless{}{-}}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{509}\NormalTok{, }\DecValTok{353}\NormalTok{, }\DecValTok{177}\NormalTok{, }\DecValTok{114}\NormalTok{,  }\DecValTok{77}\NormalTok{,  }\DecValTok{77}\NormalTok{,  }\DecValTok{53}\NormalTok{,  }\DecValTok{73}\NormalTok{,  }\DecValTok{64}\NormalTok{)}
\CommentTok{\# Concentration parameters}
\NormalTok{a \textless{}{-}}\StringTok{  }\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{9}\NormalTok{)}
\CommentTok{\# Expected proportions}
\NormalTok{p \textless{}{-}}\StringTok{ }\KeywordTok{log10}\NormalTok{((}\DecValTok{1}\OperatorTok{:}\DecValTok{9} \OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{)}\OperatorTok{/}\DecValTok{1}\OperatorTok{:}\DecValTok{9}\NormalTok{)}
\CommentTok{\# Execute the analysis}
\NormalTok{results\_H0\_He  \textless{}{-}}\StringTok{ }\KeywordTok{mult\_bf\_equality}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x, }\DataTypeTok{a =}\NormalTok{ a, }\DataTypeTok{p =}\NormalTok{ p)}
\end{Highlighting}
\end{Shaded}

Since the hypotheses \(\mathcal{H}_{r1}\) and \(\mathcal{H}_{r2}\) contain inequality constraints, we use the function \texttt{mult\_bf\_informed} to compute the Bayes factor of the informed hypotheses to the encompassing hypothesis. In this function, we need to specify (1) a vector with observed counts, (2) the informed hypothesis \(\mathcal{H}_{r1}\) or \(\mathcal{H}_{r2}\) (e.g., as character vector), (3) a vector with concentration parameters of the Dirichlet prior distribution, and (4) labels for the categories of interest (i.e., leading digits):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Labels for categories of interest}
\NormalTok{factor\_levels \textless{}{-}}\StringTok{ }\DecValTok{1}\OperatorTok{:}\DecValTok{9}
\CommentTok{\# Specifying the informed Hypothesis}
\NormalTok{Hr1 \textless{}{-}}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{\textquotesingle{}1 \textgreater{} 2 \textgreater{} 3 \textgreater{} 4 \textgreater{} 5 \textgreater{} 6 \textgreater{} 7 \textgreater{} 8 \textgreater{} 9\textquotesingle{}}\NormalTok{)}
\NormalTok{Hr2 \textless{}{-}}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{\textquotesingle{}1 \textgreater{} 2 = 3 = 4 = 5 = 6 = 7 \textgreater{} 8 \textgreater{} 9\textquotesingle{}}\NormalTok{)}
\CommentTok{\# Execute the analysis}
\NormalTok{results\_He\_Hr1 \textless{}{-}}\StringTok{ }\KeywordTok{mult\_bf\_informed}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x, }\DataTypeTok{Hr =}\NormalTok{ Hr1, }\DataTypeTok{a =}\NormalTok{ a, }
                                 \DataTypeTok{factor\_levels =}\NormalTok{ factor\_levels, }
                                 \DataTypeTok{seed =} \DecValTok{2020}\NormalTok{)}
\NormalTok{results\_He\_Hr2 \textless{}{-}}\StringTok{ }\KeywordTok{mult\_bf\_informed}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x, }\DataTypeTok{Hr =}\NormalTok{ Hr2, }\DataTypeTok{a =}\NormalTok{ a, }
                                 \DataTypeTok{factor\_levels =}\NormalTok{ factor\_levels, }
                                 \DataTypeTok{seed =} \DecValTok{2020}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{logbf \textless{}{-}}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{log}\NormalTok{(results\_H0\_He}\OperatorTok{$}\NormalTok{bf}\OperatorTok{$}\NormalTok{BF0e),}
           \KeywordTok{log}\NormalTok{(results\_H0\_He}\OperatorTok{$}\NormalTok{bf}\OperatorTok{$}\NormalTok{BF0e }\OperatorTok{*}\StringTok{ }\NormalTok{results\_He\_Hr1}\OperatorTok{$}\NormalTok{bf\_list}\OperatorTok{$}\NormalTok{bf}\OperatorTok{$}\NormalTok{BFer),}
           \KeywordTok{log}\NormalTok{(results\_H0\_He}\OperatorTok{$}\NormalTok{bf}\OperatorTok{$}\NormalTok{BF0e }\OperatorTok{*}\StringTok{ }\NormalTok{results\_He\_Hr2}\OperatorTok{$}\NormalTok{bf\_list}\OperatorTok{$}\NormalTok{bf}\OperatorTok{$}\NormalTok{BFer))}
\NormalTok{bayes\_factor\_table \textless{}{-}}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}
   \DataTypeTok{BFType =} \KeywordTok{c}\NormalTok{(}\StringTok{\textquotesingle{}LogBF0e\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}LogBF0r1\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}LogBF0r2\textquotesingle{}}\NormalTok{), }
   \DataTypeTok{LogBF  =}\NormalTok{ logbf)}
\NormalTok{bayes\_factor\_table}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     BFType     LogBF
## 1  LogBF0e -17.67150
## 2 LogBF0r1 -25.08832
## 3 LogBF0r2 154.57114
\end{verbatim}

As the evidence is extreme in all three cases, we report all Bayes factors on the log scale. The log Bayes factor \(\text{log}(\text{BF}_{0e})\) suggests extreme evidence against the hypothesis that the first digits in the Greek fiscal data follow a Benford's distribution; \(\text{log}(\text{BF}_{0e}) =\) -17.67. The log Bayes factor \(\text{log}(\text{BF}_{0r1})\) indicates extreme evidence in favor for a decreasing trend, \(\text{log}(\text{BF}_{0r1}) =\) -25.09. Even though the Bayes factor suggests extreme evidence against the hypothesis that the Greek fiscal data are an empirical dataset, there is no support the hypothesis that the data are fabricated. The log Bayes factor \(\text{log}(\text{BF}_{0r1})\) indicates extreme evidence against \(\mathcal{H}_{r2}\) with \(\text{log}(\text{BF}_{0r2}) =\) 154.57. The proportions of leading digits is best characterized by a monotonously decreasing trend, compared to all parameters varying freely (\(\text{log}(\text{BF}_{r1e}) =\) 7.42), and compared to a distribution that one could expect from fabricated data (\(\text{log}(\text{BF}_{r1r2}) =\) 180).



\begin{figure}
\centering
\includegraphics{Rpackage_paper_files/figure-latex/benford-1.pdf}
\caption{\label{fig:benford}The bargraph displays the expected proportions of leading digits according to Benford's law. The black dots indicate for the actual fiscal statistics from Greece the posterior estimates for the proportion of leading digits and the corresponding 95\% credible intervals based on the encompassing model. Only three out of nine estimates cover the expected proportions.}
\end{figure}

\hypertarget{discussion}{%
\subsubsection{Discussion}\label{discussion}}

In this example we tested the data quality of Greek fiscal data in the years 1999 to 2009 by conducting three variations of a Bayesian Benford test. More precisely, we evaluated the null hypothesis thatß Greek fiscal data conform to Benfords law. We tested this hypothesis against three alternatives. The first alternative hypothesis, \(\mathcal{H}_e\) relaxed the constraints imposed by the null hypothesis and left all model parameters free to vary. The second alternative hypothesis, \(\mathcal{H}_{r1}\) predicted a decreasing trend in the proportion of leading digits. The third alternative hypothesis \(\mathcal{H}_{r2}\) predicted a trend that Hill (1988) observed when humans tried to generate random numbers. Our result suggest that the leading digits in the fiscal statistics do not follow a Benford distribution; in fact, we collected extreme evidence against Benford's law compared to two out of three of the alternative hypotheses. When comparing the alternative hypotheses directly to each other, the data show most evidence in favor for a decreasing trend. A Benford test of fiscal statements can be a helpful tool to detect poor data quality and suspicious numbers. In follow-up checks of these numbers, it could then be examined for instance, whether financial statements were actually materially misstated, for isntance, by rounding up or down numbers, avoiding certain thresholds etc., Nigrini (2019).

\hypertarget{example-2-prevalence-of-statistical-reporting-errors}{%
\subsection{Example 2: Prevalence of Statistical Reporting Errors}\label{example-2-prevalence-of-statistical-reporting-errors}}

In any scientific article that uses null hypothesis significance testing, there is a chance that the reported test statistic and degrees of freedom do not match the reported \(p\)-value. In most cases this is because researchers copy the relevant test statistics by hand into their articles and there are no automatic checks to detect these mistakes. Therefore, Epskamp and Nuijten (2014) developed the R package \texttt{statcheck}, which only requires the PDF of a given scientific article to detect these reporting errors automatically and efficiently. This package allowed Nuijten et al. (2016) to get an overview about the prevalence of statistical reporting errors in the field of psychology. In total, the authors investigated a sample of \(30{,}717\) articles (which translates to over a quarter of a million \(p\)-values) published in eight major psychological journals between 1985 to 2013: \emph{Developmental Psychology} (DP), the \emph{Frontiers in Psychology} (FP), the \emph{Journal of Applied Psychology} (JAP), the \emph{Journal of Consulting and Clinical Psychology} (JCCP), \emph{Journal of Experimental Psychology: General} (JEPG), the \emph{Journal of Personality and Social Psychology} (JPSP), the \emph{Public Library of Science} (PLoS), \emph{Psychological Science} (PS).

Besides the overall prevalence of statistical reporting errors across these journals, the authors were interested whether there is a higher prevalence for reporting inconsistencies in certain subfields in psychology compared to others. In this context the possibility was raised that there exists a relationship between the prevalence for reporting inconsistencies and questionable research practices. Specifically, the authors argued that besides honest mistakes when transferring the test statistics into the manuscript, statistical reporting error occur when authors misreport \(p\)-values, for instance, by incorrectly rounding them down to or below \(0.05\). Based on this assumption, Nuijten et al. (2016) predicted that the proportion of statistical reporting errors should be highest in articles published in the \emph{Journal of Personality and Social Psychology} (JPSP), compared to other journals, since researchers in social psychology were shown to have the highest prevalence for questionable research practices (John, Loewenstein, \& Prelec, 2012). Specifically, John et al. (2012) found that researchers from the area of social psychology assessed questionable research practices both as more defensible and more applicable for their research compared to other research areas in psychology.

\hypertarget{data-and-hypothesis-1}{%
\subsubsection{Data and Hypothesis}\label{data-and-hypothesis-1}}

Here, we are reusing the original data published by Nuijten et al. (2016), which we also distribute with the package \textbf{multibridge} under the name \texttt{journals}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(journals)}
\end{Highlighting}
\end{Shaded}

The hypothesis of interest, \(\mathcal{H}_r\), formulated by Nuijten et al. (2016) states that the prevalence for statistical reporting errors for articles published in social psychology journals (i.e., JPSP) is higher than for articles published in other journals. Note that Nuijten et al. (2016) did not make use of inferential statistics since their sample included the entire population of articles from the eight flagship journals in psychology from 1985 to 2013. For demonstration purposes, however, we will test the informed hypothesis stated by the authors. We will test \(\mathcal{H}_r\) against the the null hypothesis \(\mathcal{H}_0\) that all journals have the same prevalence for statistical reporting errors. In this example, the parameter vector of the binomial success probabilities, \(\boldsymbol{\theta}\), reflects the probabilities of a statistical reporting error in one of the 8 journals. Thus, we can formalize the discussed hypotheses as follows:

\begin{align*}
    \mathcal{H}_r &: (\theta_{\text{DP}}, \theta_{\text{FP}}, \theta_{\text{JAP}} , \theta_{\text{JCCP}} , \theta_{\text{JEPG}} , \theta_{\text{PLoS}}, \theta_{\text{PS}}) < \theta_{\text{JPSP}} \\
    \mathcal{H}_0 &: \theta_{\text{DP}} =  \theta_{\text{FP}} =  \cdots = \theta_{\text{JPSP}}.
\end{align*}

\hypertarget{method-1}{%
\subsubsection{Method}\label{method-1}}

To compute the Bayes factor \(\text{BF}_{0r}\) we need to specify (1) a vector with observed successes (i.e., number of articles that contain a statistical reporting error), and (2) a vector containing the total number of observations, (3) the informed hypothesis, (4) a vector with prior parameter \(\alpha_i\) for each binomial proportion, (5) a vector with prior parameter \(\beta_i\) for each binomial proportion, and (6) the categories of interest (i.e., journal names). Since we have no specific expectations about the distribution of statistical reporting errors across journals, we set all parameters \(\alpha_i\) and \(\beta_i\) to one which corresponds to uniform Beta distributions. With this information, we can now conduct the analysis with the function \texttt{binom\_bf\_informed}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Since percentages are rounded to two decimal values, we round the}
\CommentTok{\# articles with an error to obtain integer values}
\NormalTok{x \textless{}{-}}\StringTok{ }\KeywordTok{round}\NormalTok{(journals}\OperatorTok{$}\NormalTok{articles\_with\_NHST  }\OperatorTok{*}\StringTok{ }
\StringTok{             }\NormalTok{(journals}\OperatorTok{$}\NormalTok{perc\_articles\_with\_errors}\OperatorTok{/}\DecValTok{100}\NormalTok{))}

\CommentTok{\# Total number of articles}
\NormalTok{n \textless{}{-}}\StringTok{ }\NormalTok{journals}\OperatorTok{$}\NormalTok{articles\_with\_NHST}

\CommentTok{\# Prior specification}
\CommentTok{\# We assign a uniform beta distribution to each binomial proportion}
\NormalTok{a \textless{}{-}}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{8}\NormalTok{)}
\NormalTok{b \textless{}{-}}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{8}\NormalTok{)}

\CommentTok{\# Specifying the informed Hypothesis}
\NormalTok{Hr \textless{}{-}}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{\textquotesingle{}JAP , PS , JCCP , PLOS , DP , FP , JEPG \textless{} JPSP\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Category labels}
\NormalTok{journal\_names \textless{}{-}}\StringTok{ }\NormalTok{journals}\OperatorTok{$}\NormalTok{journal}

\CommentTok{\# Execute the analysis}
\NormalTok{results\_H0\_Hr \textless{}{-}}\StringTok{ }\KeywordTok{binom\_bf\_informed}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x, }\DataTypeTok{n =}\NormalTok{ n, }\DataTypeTok{Hr =}\NormalTok{ Hr, }\DataTypeTok{a =}\NormalTok{ a, }\DataTypeTok{b =}\NormalTok{ b,}
                               \DataTypeTok{factor\_levels =}\NormalTok{ journal\_names,}
                               \DataTypeTok{bf\_type =} \StringTok{\textquotesingle{}BF0r\textquotesingle{}}\NormalTok{, }\DataTypeTok{seed =} \DecValTok{2020}\NormalTok{)}

\NormalTok{BFre \textless{}{-}}\StringTok{ }\NormalTok{results\_H0\_Hr}\OperatorTok{$}\NormalTok{bf\_list}\OperatorTok{$}\NormalTok{bfr\_table[}\StringTok{\textquotesingle{}BFre\textquotesingle{}}\NormalTok{]}
\NormalTok{BFe0 \textless{}{-}}\StringTok{ }\NormalTok{results\_H0\_Hr}\OperatorTok{$}\NormalTok{bf\_list}\OperatorTok{$}\NormalTok{bf0\_table[}\StringTok{\textquotesingle{}BFe0\textquotesingle{}}\NormalTok{]}
\NormalTok{BFr0 \textless{}{-}}\StringTok{ }\NormalTok{results\_H0\_Hr}\OperatorTok{$}\NormalTok{bf\_list}\OperatorTok{$}\NormalTok{bf[}\StringTok{\textquotesingle{}BFr0\textquotesingle{}}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

The Bayes factor \(\text{BF}_{r0}\) suggests extreme evidence for the informed hypothesis that the social psychology journal JPSP has the highest prevalence for statistical reporting errors compared to the null hypothesis that the statistical reporting errors are equal across journals; \(\text{BF}_{r0} = -158\).
When taking a closer look at the Bayes factors, we also see that the data suggest that the null hypothesis that the statistical reporting errors are equal across journals is highly unlikely compared to the encompassing hypothesis; with \(\text{BF}_{e0}\) of \(7.38 \times 10^{67}\). In addition, the results suggest that the data are \(7.43\) more likely under the informed hypothesis than under the hypothesis that the ordering of the journals can vary freely.

In order to get a clearer picture about the ordering of the journals, we can investigate the posterior estimates
under the encompassing model as the next step. The posterior median and 95\% credible interval are returned the \texttt{summary}-method and can be plotted, too:



\begin{figure}
\centering
\includegraphics{Rpackage_paper_files/figure-latex/journals-1.pdf}
\caption{\label{fig:journals}The figure displays for each journal the posterior estimates for the prevalence that an article includes a statistical reporting error and the corresponding 95\% credible intervals based on the encompassing model. It appears that all journals show a relatively similar prevalence for statistical reporting errors, with the exception of the \emph{Journal of Applied Psychology} (JAP) and \emph{Psychological Science} (PS), whose prevalence is much lower.}
\end{figure}

\hypertarget{discussion-1}{%
\subsubsection{Discussion}\label{discussion-1}}

In this example, we tested whether the prevalence for statistical reporting errors for articles published in social psychology journals (i.e., JPSP) is higher than for articles published in other journals. We tested this hypothesis against the null hypothesis that the prevalence for statistical reporting errors is equal across all journals. The resulting Bayes factor of \(\text{BF}_{r0} = 1.82 \times 10^{-69}\) provides extreme evidence for the informed hypothesis. However, this result should be interpreted with caution and be considered more differentiated. It seems that the result is above all an indication that the null hypothesis is highly misspecified and that the prevalence for a statistical reporting error varies greatly from journal to journal. Evidence that JPSP stands out and has a higher prevalence than the other journals is relatively small; the data provided only moderate evidence against the encompassing hypotheses.

\hypertarget{summary}{%
\section{Summary}\label{summary}}

The \texttt{R} package \textbf{multibridge} facilitates the computation of Bayes factors for informed hypotheses in multinomial models. The underlying algorithm is based on a recently developed bridge sampling routine that is more efficient and reliable than available methods. \textbf{multibridge} can evaluate hypotheses that feature equality constraints, inequality constraints, and free parameters as well as mixtures between them. The core functions of the software package were illustrated with two empirical examples. The \textbf{multibridge} package is under continuous development. In the future, we aim to implement methods that extend the functionality of the package to hierarchical binomial and multinomial models. In addition, we want to enable users to specify order constraints that are more complex, including hypotheses on the size ratios of the parameters of interest or the difference between category proportions. \todo[inline, color=myGreen]{reiterate benefits over existing packages}

\clearpage

\hypertarget{references}{%
\section{References}\label{references}}

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

\hypertarget{refs}{}
\begin{cslreferences}
\leavevmode\hypertarget{ref-benford1938law}{}%
Benford, F. (1938). The law of anomalous numbers. \emph{Proceedings of the American Philosophical Society}, 551--572.

\leavevmode\hypertarget{ref-bennett1976efficient}{}%
Bennett, C. H. (1976). Efficient estimation of free energy differences from Monte Carlo data. \emph{Journal of Computational Physics}, \emph{22}, 245--268.

\leavevmode\hypertarget{ref-damien2001sampling}{}%
Damien, P., \& Walker, S. G. (2001). Sampling truncated normal, beta, and gamma densities. \emph{Journal of Computational and Graphical Statistics}, \emph{10}, 206--215.

\leavevmode\hypertarget{ref-durtschi2004effective}{}%
Durtschi, C., Hillison, W., \& Pacini, C. (2004). The effective use of benford's law to assist in detecting fraud in accounting data. \emph{Journal of Forensic Accounting}, \emph{5}, 17--34.

\leavevmode\hypertarget{ref-epskamp2014statcheck}{}%
Epskamp, S., \& Nuijten, M. (2014). \emph{Statcheck: Extract statistics from articles and recompute p values (R package version 1.0.0.)}. Comprehensive R Archive Network. Retrieved from \url{https://cran.r-project.org/web/packages/statcheck}

\leavevmode\hypertarget{ref-europeanCommision2004}{}%
European Commision. (2004). \emph{Report by Eurostat on the revision of the Greek government deficit and debt figures} {[}Eurostat Report{]}. \url{https://ec.europa.eu/eurostat/web/products-eurostat-news/-/GREECE}.

\leavevmode\hypertarget{ref-europeanCommision2010}{}%
European Commision. (2010). \emph{Report on Greek government deficit and debt statistics} {[}Eurostat Report{]}. \url{https://ec.europa.eu/eurostat/web/products-eurostat-news/-/COM_2010_REPORT_GREEK}.

\leavevmode\hypertarget{ref-gronau2017tutorial}{}%
Gronau, Q. F., Sarafoglou, A., Matzke, D., Ly, A., Boehm, U., Marsman, M., \ldots{} Steingroever, H. (2017). A tutorial on bridge sampling. \emph{Journal of Mathematical Psychology}, \emph{81}, 80--97.

\leavevmode\hypertarget{ref-gronau2017bridgesampling}{}%
Gronau, Q. F., Singmann, H., \& Wagenmakers, E. (2020). Bridgesampling: An R package for estimating normalizing constants. \emph{Journal of Statistical Software, Articles}, \emph{92}(10), 1--29.

\leavevmode\hypertarget{ref-gu2019bain}{}%
Gu, X., Hoijtink, H., Mulder, J., \& Rosseel, Y. (2019). Bain: A program for bayesian testing of order constrained hypotheses in structural equation models. \emph{Journal of Statistical Computation and Simulation}, \emph{89}, 1526--1553.

\leavevmode\hypertarget{ref-gu2014bayesian}{}%
Gu, X., Mulder, J., Deković, M., \& Hoijtink, H. (2014). Bayesian evaluation of inequality constrained hypotheses. \emph{Psychological Methods}, \emph{19}, 511--527.

\leavevmode\hypertarget{ref-heck2019multinomial}{}%
Heck, D. W., \& Davis-Stober, C. P. (2019). Multinomial models with linear inequality constraints: Overview and improvements of computational methods for Bayesian inference. \emph{Journal of Mathematical Psychology}, \emph{91}, 70--87.

\leavevmode\hypertarget{ref-hill1988random}{}%
Hill, T. P. (1988). Random-number guessing and the first digit phenomenon. \emph{Psychological Reports}, \emph{62}, 967--971.

\leavevmode\hypertarget{ref-hill1995statistical}{}%
Hill, T. P. (1995). A statistical derivation of the significant-digit law. \emph{Statistical Science}, 354--363.

\leavevmode\hypertarget{ref-hoijtink2011informative}{}%
Hoijtink, H. (2011). \emph{Informative hypotheses: Theory and practice for behavioral and social scientists}. Boca Raton, FL: Chapman \& Hall/CRC.

\leavevmode\hypertarget{ref-hoijtink2008bayesian}{}%
Hoijtink, H., Klugkist, I., \& Boelen, P. (Eds.). (2008). \emph{Bayesian evaluation of informative hypotheses}. New York: Springer Verlag.

\leavevmode\hypertarget{ref-jeffreys1935some}{}%
Jeffreys, H. (1935). Some tests of significance, treated by the theory of probability. \emph{Proceedings of the Cambridge Philosophy Society}, \emph{31}, 203--222.

\leavevmode\hypertarget{ref-john2012measuring}{}%
John, L. K., Loewenstein, G., \& Prelec, D. (2012). Measuring the prevalence of questionable research practices with incentives for truth telling. \emph{Psychological Science}, \emph{23}, 524--532.

\leavevmode\hypertarget{ref-kass1995bayes}{}%
Kass, R. E., \& Raftery, A. E. (1995). Bayes factors. \emph{Journal of the American Statistical Association}, \emph{90}, 773--795.

\leavevmode\hypertarget{ref-klugkist2005bayesian}{}%
Klugkist, I., Kato, B., \& Hoijtink, H. (2005). Bayesian model selection using encompassing priors. \emph{Statistica Neerlandica}, \emph{59}, 57--69.

\leavevmode\hypertarget{ref-meng1996simulating}{}%
Meng, X.-L., \& Wong, W. H. (1996). Simulating ratios of normalizing constants via a simple identity: A theoretical exploration. \emph{Statistica Sinica}, \emph{6}, 831--860.

\leavevmode\hypertarget{ref-mulder2014prior}{}%
Mulder, J. (2014). Prior adjusted default Bayes factors for testing (in) equality constrained hypotheses. \emph{Computational Statistics \& Data Analysis}, \emph{71}, 448--463.

\leavevmode\hypertarget{ref-mulder2016bayes}{}%
Mulder, J. (2016). Bayes factors for testing order--constrained hypotheses on correlations. \emph{Journal of Mathematical Psychology}, \emph{72}, 104--115.

\leavevmode\hypertarget{ref-mulder2012biems}{}%
Mulder, J., Hoijtink, H., Leeuw, C. de, \& others. (2012). BIEMS: A Fortran 90 program for calculating Bayes factors for inequality and equality constrained models. \emph{Journal of Statistical Software}, \emph{46}, 1--39.

\leavevmode\hypertarget{ref-mulder2009bayesian}{}%
Mulder, J., Klugkist, I., van de Schoot, R., Meeus, W. H. J., Selfhout, M., \& Hoijtink, H. (2009). Bayesian model selection of informative hypotheses for repeated measurements. \emph{Journal of Mathematical Psychology}, \emph{53}, 530--546.

\leavevmode\hypertarget{ref-newcomb1881note}{}%
Newcomb, S. (1881). Note on the frequency of use of the different digits in natural numbers. \emph{American Journal of Mathematics}, \emph{4}, 39--40.

\leavevmode\hypertarget{ref-nigrini2012benford}{}%
Nigrini, M. (2012). \emph{Benford's Law: Applications for forensic accounting, auditing, and fraud detection} (Vol. 586). Hoboken, New Jersey: John Wiley \& Sons.

\leavevmode\hypertarget{ref-nigrini2019patterns}{}%
Nigrini, M. J. (2019). The patterns of the numbers used in occupational fraud schemes. \emph{Managerial Auditing Journal}, \emph{34}, 602--622.

\leavevmode\hypertarget{ref-nigrini1997use}{}%
Nigrini, M. J., \& Mittermaier, L. J. (1997). The use of benford's law as an aid in analytical procedures. \emph{Auditing}, \emph{16}, 52.

\leavevmode\hypertarget{ref-nuijten2016prevalence}{}%
Nuijten, M. B., Hartgerink, C. H., Assen, M. A. van, Epskamp, S., \& Wicherts, J. M. (2016). The prevalence of statistical reporting errors in psychology (1985--2013). \emph{Behavior Research Methods}, \emph{48}, 1205--1226.

\leavevmode\hypertarget{ref-overstall2010default}{}%
Overstall, A. M., \& Forster, J. J. (2010). Default Bayesian model determination methods for generalised linear mixed models. \emph{Computational Statistics \& Data Analysis}, \emph{54}, 3269--3288.

\leavevmode\hypertarget{ref-rauch2011fact}{}%
Rauch, B., Göttsche, M., Brähler, G., \& Engel, S. (2011). Fact and fiction in EU-governmental economic data. \emph{German Economic Review}, \emph{12}, 243--255.

\leavevmode\hypertarget{ref-sarafoglou2020evaluatingPreprint}{}%
Sarafoglou, A., Haaf, J. M., Ly, A., Gronau, Q. F., Wagenmakers, E., \& Marsman, M. (2020). Evaluating multinomial order restrictions with bridge sampling. \emph{PsyArXiv}. Retrieved from \url{https://psyarxiv.com/bux7p/}

\leavevmode\hypertarget{ref-stan2020}{}%
Stan Development Team. (2020). \emph{Stan modeling language user's guide and reference manual, version 2.23.0}. R Foundation for Statistical Computing. Retrieved from \url{http://mc-stan.org/}
\end{cslreferences}

\endgroup


\clearpage
\makeatletter
\efloat@restorefloats
\makeatother


\begin{appendix}
\hypertarget{transforming-an-ordered-probability-vector-to-the-real-line}{%
\section{Transforming An Ordered Probability Vector To The Real
Line}\label{transforming-an-ordered-probability-vector-to-the-real-line}}

Since we choose the multivariate normal as proposal distribution, the
mapping between the proposal and target distribution requires us to move
\(\boldsymbol{\theta}\) to the real line. Crucially this transformation
needs to retain the inequality constraints imposed on the parameters,
that is, it needs to take into account the lower bound \(l_k\) and the
upper bound \(u_k\) of each \(\theta_k\). To achieve this goal,
\textbf{multibridge} uses a probit transformation as proposed in
Sarafoglou et al. (2020) which subsequently transforms the elements in
\(\boldsymbol{\theta}\) moving from its lowest to its highest value. In
the binomial model, we move all elements in \(\boldsymbol{\theta}\) to
the real line and thus construct a new vector
\(\boldsymbol{y} \in \mathbb{R}^{K}\). For multinomial models, however,
it follows from the sum-to-one constraint that the vector
\(\boldsymbol{\theta}\) is completely determined by its first \(K - 1\)
elements of
\(\boldsymbol{\theta}: \theta_1 \leq \theta_2 \leq \cdots \leq 1 - \sum_{k = 1}^K \theta_k\).
Hence, for the transformation we will only consider the first \(K - 1\)
elements of \(\boldsymbol{\theta}\) and we will transform them to
\(K - 1\) elements of a new vector
\(\boldsymbol{y} \in \mathbb{R}^{K - 1}\).

Let \(\phi\) denote the density of a normal variable with a mean of zero
and a variance of one, \(\Phi\) denote its cumulative density function,
and \(\Phi^{-1}\) denote the inverse cumulative density function. Then
for each element \(\theta_k\), the transformation is
\[\xi_k = \Phi^{-1}\left(\frac{\theta_k - l_k}{u_k - l_k}\right),\] The
inverse transformation is given by
\[\theta_k = (u_k - l_k) \Phi(\xi_k) + l_k.\]

To perform the transformations, we thus need to determine the lower
bound \(l_k\) and the upper bound \(u_k\) of each \(\theta_k\). Assuming
\(\theta_{k-1} < \theta_{k}\) for \(k \in \{1 \cdots, K\}\) the lower
bound for any element in \(\boldsymbol{\theta}\) is defined as

\begin{align*}
l_k = \left.
\begin{cases}
0 & \text{if } k = 1 \\
\theta_{k - 1} & \text{if } 1 < k < K.
\end{cases}
\right.
\end{align*}

This definition holds for both binomial models and multinomial models.
Differences in these two models appear only when determining the upper
bound for each parameter. For binomial models, the upper bound for each
\(\theta_k\) is simply \(1\). For multinomial models, however, due to
the sum-to-one constraint the upper bounds depend on the values of
smaller elements as well as on the number of remaining larger elements
in \(\boldsymbol{\theta}\). To be able to determine the upper bounds, we
represent \(\boldsymbol{\theta}\) as unit-length stick which we
subsequently divide into \(K\) elements (Frigyik, Kapila, \& Gupta,
2010; Stan Development Team, 2020). Through this so-called
stick-breaking method the upper bound or any \(\theta_k\) is defined as
follows:

\begin{align}
\label{Eq:upperBound}
u_k = \left.
\begin{cases}
\cfrac{1}{K} & \text{if } k = 1 \\
\cfrac{1 - \sum_{i < k} \theta_i}{ERS} & \text{if } 1 < k < K,
\end{cases}
\right.
\end{align} where \(1 - \sum_{i < k} \theta_i\) represents the length of
the remaining stick, that is, the proportion of the unit-length stick
that still needs to be divided among the remaining elements in
\(\boldsymbol{\theta}\). The elements in the remaining stick are denoted
as \(ERS\), and are computed as follows: \[ERS = K - 1 + k\].

The transformations outlined above are suitable for binomial and
multinomial models featuring hypotheses in which all parameters are
inequality constrained. However, when informed hypotheses also feature
equality constrained parameters, as well as parameters that are free to
vary we need to modify the formula. Specifically, to determine the lower
bounds for each parameter, we need to take into account for each element
\(\theta_k\) the number of equality constrained parameters that are
collapsed within this element (denoted as \(e_k\)):

\begin{align}
l_k = \left.
\begin{cases}
0 & \text{if } k = 1 \\
\frac{\theta_{k - 1}}{e_{k-1}} \times e_k & \text{if } 1 < k < K,
\end{cases}
\right.
\end{align} where \(e_{k-1}\) refers to the number of equality
constrained parameters that are collapsed in \(\theta_{k - 1}\). The
upper bound for parameters in the binomial models still remains \(1\).
For multinomial models, we additionally need to take into account for
each element \(\theta_k\) the number of free parameters that share
common upper and lower bounds (denoted with \(f_k\)). The upper bound is
then defined as:

\begin{align}
u_k = \left.
\begin{cases}
\cfrac{1 - (f_k \times l_k)}{K} & \text{if } k = 1 \\
\left( \cfrac{1 - \sum_{i < k} \theta_i - (f_k \times l_k)}{ERS} \right) \times e_k & \text{if } 1 < k < K \text{ and } u_k \geq \text{max}(\theta_{i < k}), \\
\left( 2 \times \left( \cfrac{1 - \sum_{i < k} \theta_i - (f_k \times l_k)}{ERS} \right) - \text{max}(\theta_{i < k}) \right)  \times e_k & \text{if } 1 < k < K \text{ and } u_k < \text{max}(\theta_{i < k}).
\end{cases}
\right.
\end{align} The elements in the remaining stick are then computed as
follows \[ERS = e_k + \sum_{j > k} e_j \times f_j.\] The rationale
behind these modifications will be described in more detail in the
following sections. In \textbf{multibridge}, information that is
relevant for the transformation of the parameter vectors is stored in
the generated \texttt{restriction\_list} which is returned by the main
functions \texttt{binom\_bf\_informed} and \texttt{mult\_bf\_informed}
but can also be generated separately with the function
\texttt{generate\_restriction\_list}. This restriction list features the
sublist \texttt{inequality\_constraints} which encodes the number of
equality constraints collapsed in each parameter in
\texttt{nr\_mult\_equal}. Similarly the number of free parameters that
share a common bounds are encoded under \texttt{nr\_mult\_free}.

\hypertarget{equality-constrained-parameters}{%
\paragraph{Equality Constrained
Parameters}\label{equality-constrained-parameters}}

In cases where informed hypotheses feature a mix of equality and
inequality constrained parameters, we compute the corresponding Bayes
factor \(\text{BF}_{re}\), by multiplying the individual Bayes factors
for both constrait types with each other:

\[
\text{BF}_{re}
= \text{BF}_{1e} \times \text{BF}_{2e} \mid \text{BF}_{1e},
\] where the subscript \(1\) denotes the hypothesis that only features
equality constraints and the subscript \(2\) denotes the hypothesis that
only features inequality constraints. To receive
\(\text{BF}_{2e} \mid \text{BF}_{1e}\), we collapse in the constrained
prior and posterior distributions all equality constrained parameters
into one category which has implications on the performed
transformations. When transforming the samples from these distributions,
we need to account for the fact that the inequality constraints imposed
under the original parameter values might not hold for the collapsed
parameters. Consider for instance the following informed hypothesis
\[\mathcal{H}_r: \theta_1 \leq \theta_2 = \theta_3 = \theta_4 \leq \theta_5 \leq \theta_6,\]
where the elements in \(\boldsymbol{\theta}\) take the values
\((0.05, 0.15, 0.15, 0.15, 0.23, 0.27)\). Under the original parameter
values the inequality constraints hold since \(0.05\) is smaller than
\(0.15\), \(0.23\) and \(0.27\). However, the same constraint does not
hold when we collapse the categories \(\theta_2\), \(\theta_3\), and
\(\theta_4\) into \(\theta_*\). That is, the collapsed parameter
\(\theta_* = 0.45\) is now larger than \(0.23\) and \(0.27\). In
general, to determine the lower bound for a given parameter \(\theta_k\)
we thus need to take into account both the number of collapsed
categories in the preceding parameter \(e_{k-1}\) as well as the number
of collapsed categories in the current parameter \(e_{k}\). In the
example above, this means that to determine the lower bound for
\(\theta_*\) we multiply the preceding value \(\theta_1\) by three, such
that the lower bound is \(0.05 \times 3 = 0.15\). In addition, to
determine the lower bound of \(\theta_5\) we divide the preceding value
\(\theta_*\) by three, that is, \(0.6/3 = 0.2\). In general, lower
bounds for the parameters need to be adjusted as follows: \begin{align}
l_k = \left.
\begin{cases}
0 & \text{if } k = 1 \\
\frac{\theta_{k - 1}}{e_{k-1}} \times e_k & \text{if } 1 < k < K,
\end{cases}
\right.
\end{align} where \(e_{k-1}\) and \(e_k\) refer to the number of
equality constrained parameters that are collapsed in \(\theta_{k - 1}\)
and \(\theta_{k}\), respectively. Similarly, to determine the upper
bound for a given parameter value, we need to multiple the upper bound
the number of equality constrained parameters within the current
constraint:

\begin{align}
u_k = \left.
\begin{cases}
\cfrac{1}{ERS} \times e_k & \text{if } k = 1 \\
\cfrac{1 - \sum_{i < k} \theta_i}{ERS} \times e_k & \text{if } 1 < k < K,
\end{cases}
\right.
\end{align} where \(1 - \sum_{i < k} \theta_i\) represents the length of
the remaining stick and the number of elements in the remaining stick
are computed as follows: \(ERS = \sum_k^{K} e_k\). For the example
above, the upper bound for \(\theta_*\) is
\(\cfrac{1 - 0.05}{5} \times 3 = 0.57\). The upper bound for
\(\theta_5\) is then \(\cfrac{(1 - 0.05 - 0.45)}{2} = 0.25\).

\hypertarget{corrections-for-free-parameters}{%
\paragraph{Corrections for Free
Parameters}\label{corrections-for-free-parameters}}

Different adjustments are required for a sequence of inequality
constrained parameters that have shared upper and lower bounds, but can
vary freely within certain upper and lower bounds. For instance, the
hypothesis
\[\mathcal{H}_r: \theta_1 \leq \theta_2, \theta_3 \leq \theta_4\]
specifies that \(\theta_2\) and \(\theta_3\) have the shared lower bound
\(\theta_1\) and the shared upper bound \(1\), however, \(\theta_2\) can
be larger than \(\theta_3\) or vice versa. To integrate these cases
within the stick-breaking approach one must account for these potential
changes of order. For these cases, the lower bounds for the parameters
remain unchanged. However, to ensure that the stick-length for the
remaining parameters complies with the constraint, we need to subtract
from the length of the lemaining stick of the current parameter the
lower bounds of the remaining free parameters: \begin{align}
u_k = \left.
\begin{cases}
\cfrac{1 - (f_k \times l_k)}{K} & \text{if } k = 1 \\
\cfrac{1 - \sum_{i < k} \theta_i - (f_k \times l_k)}{ERS} & \text{if } 1 < k < K,
\end{cases}
\right.
\end{align}

where \(f_k\) represents the number of free parameters (i.e., free
parameters that have been not yet been accounted for) that share common
upper and lower bounds. Here, the number of elements in the remaining
stick is defined as the number of all parameters that are larger than
the current one: \(ERS = 1 + \sum_{j > k} f_j\). To illustrate this
correction, assume that the elements in \(\boldsymbol{\theta}\) take the
values \((0.15, 0.3, 0.2, 0.35)\). To compute the upper bound for
\(\theta_2\), we proceed as usual but additionally subtract from the
length of the remaining stick the lower bound for the remaining
parameters that share common bounds:
\(\cfrac{1 - 0.15 - (0.15 \times 1)}{2} = 0.35\).

A further correction is required, if a preceding free parameter (i.e., a
free parameter that was already accounted for in the stick) is larger
than the upper bound of the current parameter. For instance, in our
example the upper bound (without correction) for \(\theta_3\) would be
\(\cfrac{1 - 0.15 - 0.3}{2} = 0.275\), but the preceding free parameter
is \(0.3\). However, if we use the upper bound \(0.275\), then
\(\theta_4\) likewise would need to be \(0.275\), which would violate
the constraint, too (i.e., \(0.15 \leq 0.3, 0.275 \nleq 0.275\)). In
these cases, the upper bound needs to be corrected downwards. Thus, we
subtract the difference between the largest preceding free parameter in
the sequence with the current upper bound. Thus, if
\(u_k < \text{max}(\theta_{i < k})\), the upper bound becomes:
\begin{align}
u_k &= u_k - (\text{max}(\theta_{i < k}) - u_k) \\
&= 2 \times u_k - \text{max}(\theta_{i < k}).
\end{align} Thus, for our example the corrected upper bound for
\(\theta_3\) would become \(2*0.275 - 0.3 = 0.25\) which secures the
proper ordering for the remainder of the parameters, since \(\theta_4\)
would then be \(0.3\) which would be in accordance with the constraint,
that is, \(0.15 \leq 0.3, 0.25 \leq 0.3\).

\hypertarget{references}{%
\subsection{References}\label{references}}

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

\hypertarget{refs}{}
\begin{cslreferences}
\leavevmode\hypertarget{ref-frigyik2010introduction}{}%
Frigyik, B. A., Kapila, A., \& Gupta, M. R. (2010). \emph{Introduction
to the Dirichlet distribution and related processes}. Department of
Electrical Engineering, University of Washington.

\leavevmode\hypertarget{ref-sarafoglou2020evaluatingPreprint}{}%
Sarafoglou, A., Haaf, J. M., Ly, A., Gronau, Q. F., Wagenmakers, E., \&
Marsman, M. (2020). Evaluating multinomial order restrictions with
bridge sampling. \emph{PsyArXiv}. Retrieved from
\url{https://psyarxiv.com/bux7p/}

\leavevmode\hypertarget{ref-stan2020}{}%
Stan Development Team. (2020). \emph{Stan modeling language user's guide
and reference manual, version 2.23.0}. R Foundation for Statistical
Computing. Retrieved from \url{http://mc-stan.org/}
\end{cslreferences}

\endgroup
\end{appendix}

\end{document}
